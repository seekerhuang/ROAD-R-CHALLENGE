{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math, pdb\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from nonlocal_helper import Nonlocal\n",
    "from blocks import *\n",
    "from backbones import swin_transformer\n",
    "import modules.utils as lutils\n",
    "\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from backbones import AVA_backbone\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import numpy as np\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "from mmengine.runner import load_checkpoint\n",
    "# from mmaction.utils import get_root_logger\n",
    "from builder import BACKBONES\n",
    "\n",
    "from mmengine.logging import MMLogger\n",
    "\n",
    "\n",
    "from functools import reduce, lru_cache\n",
    "from operator import mul\n",
    "from einops import rearrange\n",
    "\n",
    "#good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good\n",
    "### Download weights from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "def conv3x3(in_planes, out_planes, stride=1, padding=1, bias=False):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv3d(in_planes, out_planes, kernel_size=(1, 3, 3), stride=(1, stride, stride),\n",
    "                     padding=(0,padding,padding), bias=bias)\n",
    "\n",
    "\n",
    "def conv1x1(in_channel, out_channel):\n",
    "    return nn.Conv3d(in_channel, out_channel, kernel_size=1, stride=1, padding=0, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good\n",
    "\n",
    "\n",
    "class ResNetFPN(nn.Module):\n",
    "\n",
    "    def __init__(self, block, args, pretrained=None,\n",
    "                 pretrained2d=True,\n",
    "                 patch_size=(2, 4, 4),\n",
    "        embed_dim=192,\n",
    "        depths=[2, 2, 18, 2],\n",
    "        num_heads=[6, 12, 24, 48],\n",
    "        window_size=(8, 7, 7),\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "                 drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.4,\n",
    "        patch_norm=True,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 frozen_stages=-1,\n",
    "                 use_checkpoint=False):\n",
    "        \n",
    "        self.inplanes = 64\n",
    "        super(ResNetFPN, self).__init__()\n",
    "    \n",
    "    \n",
    "        if args.MODEL_TYPE.startswith('SlowFast'):\n",
    "            with open('configs/ROAD.yml') as f:\n",
    "                config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "            opt = EasyDict(config)\n",
    "            print(opt)\n",
    "            self.inplanes = 64\n",
    "            super(ResNetFPN, self).__init__()\n",
    "            self.backbone = AVA_backbone(opt)\n",
    "    \n",
    "\n",
    "\n",
    "        depths=[2, 2, 18, 2]\n",
    "        num_heads=[6, 12, 24, 48]\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "\n",
    "        self.pretrained = pretrained\n",
    "        self.pretrained2d = pretrained2d\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_norm = patch_norm\n",
    "        self.frozen_stages = frozen_stages\n",
    "        self.window_size = window_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "\n",
    "\n",
    "        self.MODEL_TYPE = args.MODEL_TYPE\n",
    "        num_blocks = args.model_perms\n",
    "        non_local_inds = args.non_local_inds\n",
    "        model_3d_layers = args.model_3d_layers\n",
    "        self.num_blocks = num_blocks\n",
    "        self.non_local_inds = non_local_inds\n",
    "        self.model_3d_layers = model_3d_layers\n",
    "        self.patch_norm = True\n",
    "        self.patch_size=patch_size\n",
    "        self.window_size=window_size\n",
    "        self.num_layers=4\n",
    "        self.num_features = int(embed_dim * 2**(self.num_layers-1))\n",
    "        self.patch_embed=swin_transformer.PatchEmbed3D(patch_size=self.patch_size, in_chans=3, embed_dim=embed_dim,\n",
    "            norm_layer=nn.LayerNorm if norm_layer else None)#embed_dim 4 56 56\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(p=0.0)\n",
    "        # self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.swinaclayer1=swin_transformer.BasicLayer(dim=int(embed_dim*2**0),depth=depths[0],num_heads=num_heads[0],window_size=self.window_size,\n",
    "                                                      qkv_bias=True,drop_path=dpr[sum(depths[:0]):sum(depths[:1])],downsample=swin_transformer.PatchMerging if 0<self.num_layers-1 else None)#384 4 28 28\n",
    "        \n",
    "        # self.patchmerge1=swin_transformer.PatchMerging(dim=192)#384 4 28 28\n",
    "        \n",
    "        self.swinaclayer2=swin_transformer.BasicLayer(dim=embed_dim*2**1,depth=2,num_heads=12,window_size=(8,7,7),\n",
    "                                                      qkv_bias=True,drop_path=dpr[sum(depths[:1]):sum(depths[:2])],downsample=swin_transformer.PatchMerging if 1<self.num_layers-1 else None)\n",
    "        \n",
    "        # self.patchmerge2=swin_transformer.PatchMerging(dim=192*2)#768 4 14 14\n",
    "        \n",
    "        self.swinaclayer3=swin_transformer.BasicLayer(dim=embed_dim*2**2,depth=18,num_heads=24,window_size=(8,7,7),\n",
    "                                                      qkv_bias=True,drop_path=dpr[sum(depths[:2]):sum(depths[:3])],downsample=swin_transformer.PatchMerging if 2<self.num_layers-1 else None)#768 4 14 14\n",
    "        \n",
    "        # self.patchmerge3=swin_transformer.PatchMerging(dim=192*2**2)#1536 4 7 7\n",
    "        \n",
    "        self.swinaclayer4=swin_transformer.BasicLayer(dim=embed_dim*2**3,depth=2,num_heads=48,window_size=(8,7,7),\n",
    "                                                      qkv_bias=True,drop_path=dpr[sum(depths[:3]):sum(depths[:3 + 1])],downsample=swin_transformer.PatchMerging if 3<self.num_layers-1 else None)#1536 4 7 7\n",
    "        # self.layers.append(layer)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(self.num_features)\n",
    "\n",
    "        self._freeze_stages()\n",
    "        \n",
    "        \n",
    "        self.upcov1=nn.Conv3d(384, 256, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.upcov2=nn.ConvTranspose3d(768, 512, kernel_size=(1,2,2), stride=(1,2,2))\n",
    "        self.upcov3=nn.ConvTranspose3d(1536, 1024, kernel_size=(1,2,2), stride=(1,2,2))\n",
    "        self.upcov4=nn.Conv3d(1536, 2048, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        # self.swinlayer1=swinmodel.layers[0]\n",
    "        # self.swinlayer2=swinmodel.layers[1]\n",
    "        # self.swinlayer3=swinmodel.layers[2]\n",
    "        # self.swinlayer4=swinmodel.layers[3]\n",
    "\n",
    "        \n",
    "\n",
    "        # self.conv1 = nn.Conv3d(3, 64, kernel_size=(1,7,7), stride=(1,2,2), padding=(0,3,3),\n",
    "        #                        bias=False)\n",
    "        # self.bn1 = nn.BatchNorm3d(64)\n",
    "        # self.relu = nn.ReLU(inplace=True)\n",
    "        # self.pool1 = nn.MaxPool3d(kernel_size=(\n",
    "        #     1, 3, 3), stride=(1, 2, 2), padding=(0, 0, 0))\n",
    "        self.layer_names = []\n",
    "\n",
    "        self.layer1 = self._make_layer(\n",
    "            block, 64, num_blocks[0], temp_kernals=model_3d_layers[0], nl_inds=non_local_inds[0])\n",
    "        self.MODEL_TYPE = args.MODEL_TYPE\n",
    "        \n",
    "        # if args.model_subtype in ['C2D','RCN','CLSTM','RCLSTM','CGRU','RCGRU']:\n",
    "        #     self.pool2 = None\n",
    "        # else:\n",
    "        #     self.pool2 = nn.MaxPool3d(kernel_size=(\n",
    "        #         2, 1, 1), stride=(2, 1, 1), padding=(0, 0, 0))\n",
    "        self.pool2=None\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, 128, num_blocks[1], stride=2, temp_kernals=model_3d_layers[1], nl_inds=non_local_inds[1])\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, 256, num_blocks[2], stride=2, temp_kernals=model_3d_layers[2], nl_inds=non_local_inds[2])\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, 512, num_blocks[3], stride=2, temp_kernals=model_3d_layers[3], nl_inds=non_local_inds[3])\n",
    "\n",
    "        #self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        #self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        if self.MODEL_TYPE == 'SlowFast':\n",
    "            self.conv6 = conv3x3(2304, 256, stride=2, padding=1)  # P6\n",
    "            self.conv7 = conv3x3(256, 256, stride=2, padding=1)  # P7\n",
    "\n",
    "            self.ego_lateral = conv3x3(512 * block.expansion,  256, stride=2, padding=0)\n",
    "            self.avg_pool = nn.AdaptiveAvgPool3d((None, 1, 1))\n",
    "\n",
    "            self.lateral_layer1 = conv1x1(2304, 256)\n",
    "            self.lateral_layer2 = conv1x1(1152, 256)\n",
    "            self.lateral_layer3 = conv1x1(576, 256)\n",
    "            \n",
    "            self.corr_layer1 = conv3x3(256, 256, stride=1, padding=1)  # P4\n",
    "            self.corr_layer2 = conv3x3(256, 256, stride=1, padding=1)  # P4\n",
    "            self.corr_layer3 = conv3x3(256, 256, stride=1, padding=1)  # P3\n",
    "        else:\n",
    "            self.conv6 = conv3x3(512 * block.expansion, 256, stride=2, padding=1)  # P6\n",
    "            self.conv7 = conv3x3(256, 256, stride=2, padding=1)  # P7\n",
    "\n",
    "            # self.ego_lateral = conv3x3(512 * block.expansion,  256, stride=2, padding=0)\n",
    "            self.avg_pool = nn.AdaptiveAvgPool3d((None, 1, 1))\n",
    "\n",
    "            self.lateral_layer1 = conv1x1(512 * block.expansion, 256)\n",
    "            self.lateral_layer2 = conv1x1(256 * block.expansion, 256)\n",
    "            self.lateral_layer3 = conv1x1(128 * block.expansion, 256)\n",
    "            \n",
    "            self.corr_layer1 = conv3x3(256, 256, stride=1, padding=1)  # P4\n",
    "            self.corr_layer2 = conv3x3(256, 256, stride=1, padding=1)  # P4\n",
    "            self.corr_layer3 = conv3x3(256, 256, stride=1, padding=1)  # P3\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_uniform_(m.weight, a=1)\n",
    "                if hasattr(m.bias, 'data'):\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def _freeze_stages(self):\n",
    "        if self.frozen_stages >= 0:\n",
    "            self.patch_embed.eval()\n",
    "            for param in self.patch_embed.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 1:\n",
    "            self.pos_drop.eval()\n",
    "            for i in range(0, self.frozen_stages):\n",
    "                m = self.layers[i]\n",
    "                m.eval()\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _upsample(self, x, y):\n",
    "        _, _, t, h, w = y.size()\n",
    "        # print('spatial', x.shape, y.shape)\n",
    "        x_upsampled = F.interpolate(x, [t, h, w], mode='nearest')\n",
    "\n",
    "        return x_upsampled\n",
    "\n",
    "    def _upsample_time(self, x):\n",
    "        _,_,t, h, w = x.size()\n",
    "        # print('time', x.shape)\n",
    "        x_upsampled = F.interpolate(x, [t*2, h, w], mode='nearest')\n",
    "\n",
    "        return x_upsampled\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride=1, temp_kernals=[], nl_inds=[]):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=(1, stride, stride), bias=False),\n",
    "                nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layer_names = []\n",
    "        if 0 in temp_kernals:\n",
    "            layers.append(block(self.inplanes, planes, stride,\n",
    "                                temp_kernal=3, downsample=downsample))\n",
    "        else:\n",
    "            layers.append(block(self.inplanes, planes, stride,\n",
    "                                temp_kernal=0, downsample=downsample))\n",
    "        layer_names.append(0)\n",
    "        self.inplanes = planes * block.expansion\n",
    "\n",
    "        for i in range(1, num_blocks):\n",
    "            if i in temp_kernals:\n",
    "                layers.append(block(self.inplanes, planes, temp_kernal=3))\n",
    "            else:\n",
    "                layers.append(block(self.inplanes, planes, temp_kernal=1))\n",
    "            layer_names.append(i)\n",
    "\n",
    "            if i in nl_inds:\n",
    "                nln = Nonlocal(\n",
    "                    planes * block.expansion,\n",
    "                    (planes * block.expansion) // 2,\n",
    "                    [1, 2, 2],\n",
    "                    instantiation='softmax',\n",
    "                )\n",
    "                layers.append(nln)\n",
    "\n",
    "        self.layer_names.append(layer_names)\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.MODEL_TYPE.startswith('SlowFast'):\n",
    "            ff = self.backbone(x)\n",
    "            c3 = ff[0]\n",
    "            c4 = ff[1]\n",
    "            c5 = ff[2]\n",
    "            p5 = self.lateral_layer1(c5)\n",
    "            p5_upsampled = self._upsample(p5, c4)\n",
    "            p5 = self.corr_layer1(p5)\n",
    "            p4 = self.lateral_layer2(c4)\n",
    "            p4 = p5_upsampled + p4\n",
    "            p4_upsampled = self._upsample(p4, c3)\n",
    "            p4 = self.corr_layer2(p4)\n",
    "            p3 = self.lateral_layer3(c3)\n",
    "            p3 = p4_upsampled + p3\n",
    "            p3 = self.corr_layer3(p3)\n",
    "            p6 = self.conv6(c5)\n",
    "            p7 = self.conv7(F.relu(p6))\n",
    "            features = [p3, p4, p5, p6, p7]\n",
    "            ego_feat = self.avg_pool(p7)\n",
    "            if self.pool2 is not None:\n",
    "                for i in range(len(features)):\n",
    "                    features[i] = self._upsample_time(features[i])\n",
    "                ego_feat = self._upsample_time(ego_feat)\n",
    "        else:\n",
    "            x=self.patch_embed(x)\n",
    "            # print(x.shape)\n",
    "            x=self.pos_drop(x)\n",
    "            # print(x.shape)\n",
    "            x=self.swinaclayer1(x)\n",
    "            # print(x.shape)\n",
    "            x1=x #192 4 56 56\n",
    "            # x=self.patchmerge1(x)\n",
    "            x=self.swinaclayer2(x)\n",
    "            x2=x #384 4 28 28\n",
    "            # x=self.patchmerge2(x)\n",
    "            x=self.swinaclayer3(x)\n",
    "            x3=x #768 4 14 14\n",
    "            # x=self.patchmerge3(x)\n",
    "            x=self.swinaclayer4(x)\n",
    "            x = rearrange(x, 'n c d h w -> n d h w c')\n",
    "            x = self.norm3(x)\n",
    "            x = rearrange(x, 'n d h w c -> n c d h w')\n",
    "            x4=x #1536 4 7 7\n",
    "\n",
    "            #up can be pretrained\n",
    "\n",
    "            u1=self.upcov1(x1)#256 4 56 56\n",
    "            u2=self.upcov2(x2)#512 4 28 28\n",
    "            u3=self.upcov3(x3)#1024 4 14 14\n",
    "            u4=self.upcov4(x4)#2048 4 7 7\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # x = self.conv1(x)\n",
    "            # x = self.bn1(x)\n",
    "            # x = self.relu(x)\n",
    "            # x = self.pool1(x)\n",
    "            x = u1\n",
    "            # if self.pool2 is not None:\n",
    "            #     x = self.pool2(x)\n",
    "            c3 = u2\n",
    "            c4 = u3\n",
    "            c5 = u4\n",
    "\n",
    "            #down can be pretrained\n",
    "\n",
    "            p5 = self.lateral_layer1(c5)\n",
    "            p5_upsampled = self._upsample(p5, c4)\n",
    "            p5 = self.corr_layer1(p5)\n",
    "            p4 = self.lateral_layer2(c4)\n",
    "            p4 = p5_upsampled + p4\n",
    "            p4_upsampled = self._upsample(p4, c3)\n",
    "            p4 = self.corr_layer2(p4)\n",
    "            p3 = self.lateral_layer3(c3)\n",
    "            p3 = p4_upsampled + p3\n",
    "            p3 = self.corr_layer3(p3)\n",
    "            p6 = self.conv6(c5)\n",
    "            p7 = self.conv7(F.relu(p6))\n",
    "            features = [p3, p4, p5, p6, p7]\n",
    "            # ego_feat = self.avg_pool(p7)\n",
    "            if self.pool2 is not None:\n",
    "                for i in range(len(features)):\n",
    "                    features[i] = self._upsample_time(features[i])\n",
    "                # ego_feat = self._upsample_time(ego_feat)\n",
    "        return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good\n",
    "\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# import data.transforms as vtf\n",
    "# from data import VideoDataset\n",
    "# from gen_dets import gen_dets\n",
    "# from retinanet import build_retinanet\n",
    "# from modules import utils\n",
    "# from train import train\n",
    "# from tubes import build_eval_tubes\n",
    "# from utils_ssl import save_ulb_indices\n",
    "# from val import val\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "\n",
    "def seed_everything(args):\n",
    "    random.seed(args.MAN_SEED)\n",
    "    np.random.seed(args.MAN_SEED)\n",
    "    torch.manual_seed(args.MAN_SEED)\n",
    "    torch.cuda.manual_seed_all(args.MAN_SEED)\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training single stage FPN with OHEM, resnet as backbone')\n",
    "# parser.add_argument('TASK', type=int, choices=[1,2], help=\"if task 1, using only 3 out of 15 videos from the training parition; if task 2, using all 15 videos from the training partition train_1\")\n",
    "# parser.add_argument('DATA_ROOT', help='Location to root directory for dataset reading') # /mnt/mars-fast/datasets/\n",
    "# parser.add_argument('SAVE_ROOT', help='Location to root directory for saving checkpoint models') # /mnt/mars-alpha/\n",
    "# parser.add_argument('MODEL_PATH',help='Location to root directory where kinetics pretrained models are stored')\n",
    "\n",
    "parser.add_argument('--MODE', default='train', choices=['train', 'gen_dets', 'eval_frames', 'val'],\n",
    "                    help='MODE can be train, gen_dets, eval_frames, val; define SUBSETS accordingly, build tubes')\n",
    "# Name of backbone network, e.g. resnet18, resnet34, resnet50, resnet101 resnet152 are supported\n",
    "parser.add_argument('--ARCH', default='resnet50', \n",
    "                    type=str, help=' base arch')\n",
    "parser.add_argument('--MODEL_TYPE', default='RCGRU',\n",
    "                    type=str, help=' base model')\n",
    "parser.add_argument('--ANCHOR_TYPE', default='RETINA',\n",
    "                    type=str, help='type of anchors to be used in model')\n",
    "\n",
    "parser.add_argument('--SEQ_LEN', default=8,\n",
    "                    type=int, help='NUmber of input frames')\n",
    "parser.add_argument('--TEST_SEQ_LEN', default=8,\n",
    "                    type=int, help='NUmber of input frames')\n",
    "parser.add_argument('--MIN_SEQ_STEP', default=1,\n",
    "                    type=int, help='DIFFERENCE of gap between the frames of sequence')\n",
    "parser.add_argument('--MAX_SEQ_STEP', default=1,\n",
    "                    type=int, help='DIFFERENCE of gap between the frames of sequence')\n",
    "# if output heads are have shared features or not: 0 is no-shareing else sharining enabled\n",
    "# parser.add_argument('--MULIT_SCALE', default=False, type=str2bool,help='perfrom multiscale training')\n",
    "parser.add_argument('--HEAD_LAYERS', default=3, \n",
    "                    type=int,help='0 mean no shareding more than 0 means shareing')\n",
    "parser.add_argument('--NUM_FEATURE_MAPS', default=5, \n",
    "                    type=int,help='0 mean no shareding more than 0 means shareing')\n",
    "parser.add_argument('--CLS_HEAD_TIME_SIZE', default=3, \n",
    "                    type=int, help='Temporal kernel size of classification head')\n",
    "parser.add_argument('--REG_HEAD_TIME_SIZE', default=3,\n",
    "                type=int, help='Temporal kernel size of regression head')\n",
    "\n",
    "#  Name of the dataset only voc or coco are supported\n",
    "parser.add_argument('--DATASET', default='road', \n",
    "                    type=str,help='dataset being used')\n",
    "parser.add_argument('--TRAIN_SUBSETS', default='train_1,',\n",
    "                    type=str,help='Training SUBSETS seprated by ,')\n",
    "parser.add_argument('--VAL_SUBSETS', default='val_1',\n",
    "                    type=str,help='Validation SUBSETS seprated by ,')\n",
    "parser.add_argument('--TEST_SUBSETS', default='', \n",
    "                    type=str,help='Testing SUBSETS seprated by ,')\n",
    "# Input size of image only 600 is supprted at the moment \n",
    "parser.add_argument('--MIN_SIZE', default=512, \n",
    "                    type=int, help='Input Size for FPN')\n",
    "\n",
    "#  data loading argumnets\n",
    "parser.add_argument('-b','--BATCH_SIZE', default=4, \n",
    "                    type=int, help='Batch size for training')\n",
    "parser.add_argument('--TEST_BATCH_SIZE', default=1, \n",
    "                    type=int, help='Batch size for testing')\n",
    "# Number of worker to load data in parllel\n",
    "parser.add_argument('--NUM_WORKERS', '-j', default=8, \n",
    "                    type=int, help='Number of workers used in dataloading')\n",
    "# optimiser hyperparameters\n",
    "parser.add_argument('--OPTIM', default='SGD', \n",
    "                    type=str, help='Optimiser type')\n",
    "parser.add_argument('--RESUME', default=0, \n",
    "                    type=int, help='Resume from given epoch')\n",
    "parser.add_argument('--MAX_EPOCHS', default=150,\n",
    "                    type=int, help='Number of training epoc')\n",
    "parser.add_argument('-l','--LR', '--learning-rate', \n",
    "                    default=0.0041, type=float, help='initial learning rate')\n",
    "parser.add_argument('--MOMENTUM', default=0.9, \n",
    "                    type=float, help='momentum')\n",
    "parser.add_argument('--MILESTONES', default='130,145',\n",
    "                    type=str, help='Chnage the lr @')\n",
    "parser.add_argument('--GAMMA', default=0.1, \n",
    "                    type=float, help='Gamma update for SGD')\n",
    "parser.add_argument('--WEIGHT_DECAY', default=1e-4, \n",
    "                    type=float, help='Weight decay for SGD')\n",
    "\n",
    "# Freeze layers or not \n",
    "parser.add_argument('--FBN','--FREEZE_BN', default=True, \n",
    "                    type=str2bool, help='freeze bn layers if true or else keep updating bn layers')\n",
    "parser.add_argument('--FREEZE_UPTO', default=1, \n",
    "                    type=int, help='layer group number in ResNet up to which needs to be frozen')\n",
    "\n",
    "# Loss function matching threshold\n",
    "parser.add_argument('--POSTIVE_THRESHOLD', default=0.5, \n",
    "                    type=float, help='Min threshold for Jaccard index for matching')\n",
    "parser.add_argument('--NEGTIVE_THRESHOLD', default=0.4,\n",
    "                    type=float, help='Max threshold Jaccard index for matching')\n",
    "# Evaluation hyperparameters\n",
    "parser.add_argument('--EVAL_EPOCHS', default='150',\n",
    "                    type=str, help='eval epochs to test network on these epoch checkpoints usually the last epoch is used')\n",
    "parser.add_argument('--VAL_STEP', default=10,\n",
    "                    type=int, help='Number of training epoch before evaluation')\n",
    "parser.add_argument('--IOU_THRESH', default=0.5, \n",
    "                    type=float, help='Evaluation threshold for validation and for frame-wise mAP')\n",
    "parser.add_argument('--CONF_THRESH', default=0.025, \n",
    "                    type=float, help='Confidence threshold for to remove detection below given number')\n",
    "parser.add_argument('--NMS_THRESH', default=0.5, \n",
    "                    type=float, help='NMS threshold to apply nms at the time of validation')\n",
    "parser.add_argument('--TOPK', default=10, \n",
    "                    type=int, help='topk detection to keep for evaluation')\n",
    "parser.add_argument('--GEN_CONF_THRESH', default=0.025, \n",
    "                    type=float, help='Confidence threshold at the time of generation and dumping')\n",
    "parser.add_argument('--GEN_TOPK', default=100, \n",
    "                    type=int, help='topk at the time of generation')\n",
    "parser.add_argument('--GEN_NMS', default=0.5, \n",
    "                    type=float, help='NMS at the time of generation')\n",
    "parser.add_argument('--CLASSWISE_NMS', default=False, \n",
    "                    type=str2bool, help='apply classwise NMS/no tested properly')\n",
    "parser.add_argument('--JOINT_4M_MARGINALS', default=False, \n",
    "                    type=str2bool, help='generate score of joints i.e. duplexes or triplet by marginals like agents and actions scores')\n",
    "\n",
    "## paths hyper parameters\n",
    "parser.add_argument('--COMPUTE_PATHS', default=False, \n",
    "                    type=str2bool, help=' COMPUTE_PATHS if set true then it overwrite existing ones')\n",
    "parser.add_argument('--PATHS_IOUTH', default=0.5,\n",
    "                    type=float, help='Iou threshold for building paths to limit neighborhood search')\n",
    "parser.add_argument('--PATHS_COST_TYPE', default='score',\n",
    "                    type=str, help='cost function type to use for matching, other options are scoreiou, iou')\n",
    "parser.add_argument('--PATHS_JUMP_GAP', default=4,\n",
    "                    type=int, help='GAP allowed for a tube to be kept alive after no matching detection found')\n",
    "parser.add_argument('--PATHS_MIN_LEN', default=6,\n",
    "                    type=int, help='minimum length of generated path')\n",
    "parser.add_argument('--PATHS_MINSCORE', default=0.1,\n",
    "                    type=float, help='minimum score a path should have over its length')\n",
    "\n",
    "## paths hyper parameters\n",
    "parser.add_argument('--COMPUTE_TUBES', default=False, type=str2bool, help='if set true then it overwrite existing tubes')\n",
    "parser.add_argument('--TUBES_ALPHA', default=0,\n",
    "                    type=float, help='alpha cost for changeing the label')\n",
    "parser.add_argument('--TRIM_METHOD', default='none',\n",
    "                    type=str, help='other one is indiv which works for UCF24')\n",
    "parser.add_argument('--TUBES_TOPK', default=10,\n",
    "                    type=int, help='Number of labels to assign for a tube')\n",
    "parser.add_argument('--TUBES_MINLEN', default=5,\n",
    "                    type=int, help='minimum length of a tube')\n",
    "parser.add_argument('--TUBES_EVAL_THRESHS', default='0.2,0.5',\n",
    "                    type=str, help='evaluation threshold for checking tube overlap at evaluation time, one can provide as many as one wants')\n",
    "# parser.add_argument('--TRAIL_ID', default=0,\n",
    "#                     type=int, help='eval TUBES_Thtrshold at evaluation time')\n",
    "\n",
    "###\n",
    "parser.add_argument('--LOG_START', default=10, \n",
    "                    type=int, help='start loging after k steps for text/tensorboard') \n",
    "parser.add_argument('--LOG_STEP', default=10, \n",
    "                    type=int, help='Log every k steps for text/tensorboard')\n",
    "parser.add_argument('--TENSORBOARD', default=1,\n",
    "                    type=str2bool, help='Use tensorboard for loss/evalaution visualization')\n",
    "\n",
    "# Program arguments\n",
    "parser.add_argument('--MAN_SEED', default=123, \n",
    "                    type=int, help='manualseed for reproduction')\n",
    "parser.add_argument('--MULTI_GPUS', default=True, type=str2bool, help='If  more than 0 then use all visible GPUs by default only one GPU used ') \n",
    "\n",
    "parser.add_argument('--LOGIC', default='None', type=str, help='T-norm to be used in the loss')\n",
    "parser.add_argument('--req_loss_weight', default=0.0, type=float, help='weight for the logic-based loss')\n",
    "parser.add_argument('--DEBUG_num_iter', default=0, type=int, help='num iterations to run; fast debugging')\n",
    "parser.add_argument('--EXP_NAME', default=\"\", type=str, help=\"Custom experiment name, for resuming and eval, provide the full path to the experiment directory\")\n",
    "\n",
    "parser.add_argument('--pretrained_model_path', default=None, type=str, help='path to pretrained model; e.g. use this to add warm-up to neurosymbolic training with tnorm based loss')\n",
    "parser.add_argument('--unlabelled_proportion', default=0.0, type=float)\n",
    "parser.add_argument('--agentness_th', default=0.125, type=float, help='threshold to distinguish foreground vs background boxes when computing t-norm')\n",
    "parser.add_argument('--model_perms',default=[3, 4, 6, 3],type=list,help='model perms')\n",
    "parser.add_argument('--model_3d_layers',default=[[0, 1, 2], [0, 2], [0, 2, 4], [0, 1]],type=list,help='model 3d layers')\n",
    "parser.add_argument('--non_local_inds',default=[[], [], [], []],type=list,help='non local inds')\n",
    "parser.add_argument('--num_classes',default=25,type=int,help='num class')\n",
    "parser.add_argument('--num_classes_list',default=[1, 24],type=list,help='class list')\n",
    "parser.add_argument('--label_types',default=['action_ness', 'action'],type=list,help='label types')\n",
    "parser.add_argument('--num_label_types',default=2,type=int,help='num label types')\n",
    "train_dataset = None\n",
    "ulb_train_dataset = None\n",
    "\n",
    "# args.model_perms = [3, 4, 6, 3]\n",
    "# args.model_3d_layers = [[0, 1, 2], [0, 2], [0, 2, 4], [0, 1]]\n",
    "# args.non_local_inds=[[], [], [], []]\n",
    "# assert args.ARCH in modelperms, 'Arch shoudl from::>' + \\\n",
    "#     ','.join([m for m in modelperms])\n",
    "\n",
    "## Parse arguments\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "import torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "args.model_perms = [3, 4, 6, 3]\n",
    "args.num_classes=25\n",
    "args.model_3d_layers = [[0, 1, 2], [0, 2], [0, 2, 4], [0, 1]]\n",
    "args.non_local_inds=[[], [], [], []]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def exract():\n",
    "# 查看backbone部分的预训练模型键值对\n",
    "    backbone_model_path = 'F:\\\\gta5\\\\swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb_20220930-f8d74db7.pth'\n",
    "    backbone_train_model = torch.load(backbone_model_path)\n",
    "    state_dict = collections.OrderedDict(backbone_train_model['state_dict'])\n",
    "    print(list(state_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['backbone.patch_embed.proj.weight', 'backbone.patch_embed.proj.bias', 'backbone.patch_embed.norm.weight', 'backbone.patch_embed.norm.bias', 'backbone.layers.0.blocks.0.norm1.weight', 'backbone.layers.0.blocks.0.norm1.bias', 'backbone.layers.0.blocks.0.attn.relative_position_bias_table', 'backbone.layers.0.blocks.0.attn.relative_position_index', 'backbone.layers.0.blocks.0.attn.qkv.weight', 'backbone.layers.0.blocks.0.attn.qkv.bias', 'backbone.layers.0.blocks.0.attn.proj.weight', 'backbone.layers.0.blocks.0.attn.proj.bias', 'backbone.layers.0.blocks.0.norm2.weight', 'backbone.layers.0.blocks.0.norm2.bias', 'backbone.layers.0.blocks.0.mlp.fc1.weight', 'backbone.layers.0.blocks.0.mlp.fc1.bias', 'backbone.layers.0.blocks.0.mlp.fc2.weight', 'backbone.layers.0.blocks.0.mlp.fc2.bias', 'backbone.layers.0.blocks.1.norm1.weight', 'backbone.layers.0.blocks.1.norm1.bias', 'backbone.layers.0.blocks.1.attn.relative_position_bias_table', 'backbone.layers.0.blocks.1.attn.relative_position_index', 'backbone.layers.0.blocks.1.attn.qkv.weight', 'backbone.layers.0.blocks.1.attn.qkv.bias', 'backbone.layers.0.blocks.1.attn.proj.weight', 'backbone.layers.0.blocks.1.attn.proj.bias', 'backbone.layers.0.blocks.1.norm2.weight', 'backbone.layers.0.blocks.1.norm2.bias', 'backbone.layers.0.blocks.1.mlp.fc1.weight', 'backbone.layers.0.blocks.1.mlp.fc1.bias', 'backbone.layers.0.blocks.1.mlp.fc2.weight', 'backbone.layers.0.blocks.1.mlp.fc2.bias', 'backbone.layers.0.downsample.reduction.weight', 'backbone.layers.0.downsample.norm.weight', 'backbone.layers.0.downsample.norm.bias', 'backbone.layers.1.blocks.0.norm1.weight', 'backbone.layers.1.blocks.0.norm1.bias', 'backbone.layers.1.blocks.0.attn.relative_position_bias_table', 'backbone.layers.1.blocks.0.attn.relative_position_index', 'backbone.layers.1.blocks.0.attn.qkv.weight', 'backbone.layers.1.blocks.0.attn.qkv.bias', 'backbone.layers.1.blocks.0.attn.proj.weight', 'backbone.layers.1.blocks.0.attn.proj.bias', 'backbone.layers.1.blocks.0.norm2.weight', 'backbone.layers.1.blocks.0.norm2.bias', 'backbone.layers.1.blocks.0.mlp.fc1.weight', 'backbone.layers.1.blocks.0.mlp.fc1.bias', 'backbone.layers.1.blocks.0.mlp.fc2.weight', 'backbone.layers.1.blocks.0.mlp.fc2.bias', 'backbone.layers.1.blocks.1.norm1.weight', 'backbone.layers.1.blocks.1.norm1.bias', 'backbone.layers.1.blocks.1.attn.relative_position_bias_table', 'backbone.layers.1.blocks.1.attn.relative_position_index', 'backbone.layers.1.blocks.1.attn.qkv.weight', 'backbone.layers.1.blocks.1.attn.qkv.bias', 'backbone.layers.1.blocks.1.attn.proj.weight', 'backbone.layers.1.blocks.1.attn.proj.bias', 'backbone.layers.1.blocks.1.norm2.weight', 'backbone.layers.1.blocks.1.norm2.bias', 'backbone.layers.1.blocks.1.mlp.fc1.weight', 'backbone.layers.1.blocks.1.mlp.fc1.bias', 'backbone.layers.1.blocks.1.mlp.fc2.weight', 'backbone.layers.1.blocks.1.mlp.fc2.bias', 'backbone.layers.1.downsample.reduction.weight', 'backbone.layers.1.downsample.norm.weight', 'backbone.layers.1.downsample.norm.bias', 'backbone.layers.2.blocks.0.norm1.weight', 'backbone.layers.2.blocks.0.norm1.bias', 'backbone.layers.2.blocks.0.attn.relative_position_bias_table', 'backbone.layers.2.blocks.0.attn.relative_position_index', 'backbone.layers.2.blocks.0.attn.qkv.weight', 'backbone.layers.2.blocks.0.attn.qkv.bias', 'backbone.layers.2.blocks.0.attn.proj.weight', 'backbone.layers.2.blocks.0.attn.proj.bias', 'backbone.layers.2.blocks.0.norm2.weight', 'backbone.layers.2.blocks.0.norm2.bias', 'backbone.layers.2.blocks.0.mlp.fc1.weight', 'backbone.layers.2.blocks.0.mlp.fc1.bias', 'backbone.layers.2.blocks.0.mlp.fc2.weight', 'backbone.layers.2.blocks.0.mlp.fc2.bias', 'backbone.layers.2.blocks.1.norm1.weight', 'backbone.layers.2.blocks.1.norm1.bias', 'backbone.layers.2.blocks.1.attn.relative_position_bias_table', 'backbone.layers.2.blocks.1.attn.relative_position_index', 'backbone.layers.2.blocks.1.attn.qkv.weight', 'backbone.layers.2.blocks.1.attn.qkv.bias', 'backbone.layers.2.blocks.1.attn.proj.weight', 'backbone.layers.2.blocks.1.attn.proj.bias', 'backbone.layers.2.blocks.1.norm2.weight', 'backbone.layers.2.blocks.1.norm2.bias', 'backbone.layers.2.blocks.1.mlp.fc1.weight', 'backbone.layers.2.blocks.1.mlp.fc1.bias', 'backbone.layers.2.blocks.1.mlp.fc2.weight', 'backbone.layers.2.blocks.1.mlp.fc2.bias', 'backbone.layers.2.blocks.2.norm1.weight', 'backbone.layers.2.blocks.2.norm1.bias', 'backbone.layers.2.blocks.2.attn.relative_position_bias_table', 'backbone.layers.2.blocks.2.attn.relative_position_index', 'backbone.layers.2.blocks.2.attn.qkv.weight', 'backbone.layers.2.blocks.2.attn.qkv.bias', 'backbone.layers.2.blocks.2.attn.proj.weight', 'backbone.layers.2.blocks.2.attn.proj.bias', 'backbone.layers.2.blocks.2.norm2.weight', 'backbone.layers.2.blocks.2.norm2.bias', 'backbone.layers.2.blocks.2.mlp.fc1.weight', 'backbone.layers.2.blocks.2.mlp.fc1.bias', 'backbone.layers.2.blocks.2.mlp.fc2.weight', 'backbone.layers.2.blocks.2.mlp.fc2.bias', 'backbone.layers.2.blocks.3.norm1.weight', 'backbone.layers.2.blocks.3.norm1.bias', 'backbone.layers.2.blocks.3.attn.relative_position_bias_table', 'backbone.layers.2.blocks.3.attn.relative_position_index', 'backbone.layers.2.blocks.3.attn.qkv.weight', 'backbone.layers.2.blocks.3.attn.qkv.bias', 'backbone.layers.2.blocks.3.attn.proj.weight', 'backbone.layers.2.blocks.3.attn.proj.bias', 'backbone.layers.2.blocks.3.norm2.weight', 'backbone.layers.2.blocks.3.norm2.bias', 'backbone.layers.2.blocks.3.mlp.fc1.weight', 'backbone.layers.2.blocks.3.mlp.fc1.bias', 'backbone.layers.2.blocks.3.mlp.fc2.weight', 'backbone.layers.2.blocks.3.mlp.fc2.bias', 'backbone.layers.2.blocks.4.norm1.weight', 'backbone.layers.2.blocks.4.norm1.bias', 'backbone.layers.2.blocks.4.attn.relative_position_bias_table', 'backbone.layers.2.blocks.4.attn.relative_position_index', 'backbone.layers.2.blocks.4.attn.qkv.weight', 'backbone.layers.2.blocks.4.attn.qkv.bias', 'backbone.layers.2.blocks.4.attn.proj.weight', 'backbone.layers.2.blocks.4.attn.proj.bias', 'backbone.layers.2.blocks.4.norm2.weight', 'backbone.layers.2.blocks.4.norm2.bias', 'backbone.layers.2.blocks.4.mlp.fc1.weight', 'backbone.layers.2.blocks.4.mlp.fc1.bias', 'backbone.layers.2.blocks.4.mlp.fc2.weight', 'backbone.layers.2.blocks.4.mlp.fc2.bias', 'backbone.layers.2.blocks.5.norm1.weight', 'backbone.layers.2.blocks.5.norm1.bias', 'backbone.layers.2.blocks.5.attn.relative_position_bias_table', 'backbone.layers.2.blocks.5.attn.relative_position_index', 'backbone.layers.2.blocks.5.attn.qkv.weight', 'backbone.layers.2.blocks.5.attn.qkv.bias', 'backbone.layers.2.blocks.5.attn.proj.weight', 'backbone.layers.2.blocks.5.attn.proj.bias', 'backbone.layers.2.blocks.5.norm2.weight', 'backbone.layers.2.blocks.5.norm2.bias', 'backbone.layers.2.blocks.5.mlp.fc1.weight', 'backbone.layers.2.blocks.5.mlp.fc1.bias', 'backbone.layers.2.blocks.5.mlp.fc2.weight', 'backbone.layers.2.blocks.5.mlp.fc2.bias', 'backbone.layers.2.blocks.6.norm1.weight', 'backbone.layers.2.blocks.6.norm1.bias', 'backbone.layers.2.blocks.6.attn.relative_position_bias_table', 'backbone.layers.2.blocks.6.attn.relative_position_index', 'backbone.layers.2.blocks.6.attn.qkv.weight', 'backbone.layers.2.blocks.6.attn.qkv.bias', 'backbone.layers.2.blocks.6.attn.proj.weight', 'backbone.layers.2.blocks.6.attn.proj.bias', 'backbone.layers.2.blocks.6.norm2.weight', 'backbone.layers.2.blocks.6.norm2.bias', 'backbone.layers.2.blocks.6.mlp.fc1.weight', 'backbone.layers.2.blocks.6.mlp.fc1.bias', 'backbone.layers.2.blocks.6.mlp.fc2.weight', 'backbone.layers.2.blocks.6.mlp.fc2.bias', 'backbone.layers.2.blocks.7.norm1.weight', 'backbone.layers.2.blocks.7.norm1.bias', 'backbone.layers.2.blocks.7.attn.relative_position_bias_table', 'backbone.layers.2.blocks.7.attn.relative_position_index', 'backbone.layers.2.blocks.7.attn.qkv.weight', 'backbone.layers.2.blocks.7.attn.qkv.bias', 'backbone.layers.2.blocks.7.attn.proj.weight', 'backbone.layers.2.blocks.7.attn.proj.bias', 'backbone.layers.2.blocks.7.norm2.weight', 'backbone.layers.2.blocks.7.norm2.bias', 'backbone.layers.2.blocks.7.mlp.fc1.weight', 'backbone.layers.2.blocks.7.mlp.fc1.bias', 'backbone.layers.2.blocks.7.mlp.fc2.weight', 'backbone.layers.2.blocks.7.mlp.fc2.bias', 'backbone.layers.2.blocks.8.norm1.weight', 'backbone.layers.2.blocks.8.norm1.bias', 'backbone.layers.2.blocks.8.attn.relative_position_bias_table', 'backbone.layers.2.blocks.8.attn.relative_position_index', 'backbone.layers.2.blocks.8.attn.qkv.weight', 'backbone.layers.2.blocks.8.attn.qkv.bias', 'backbone.layers.2.blocks.8.attn.proj.weight', 'backbone.layers.2.blocks.8.attn.proj.bias', 'backbone.layers.2.blocks.8.norm2.weight', 'backbone.layers.2.blocks.8.norm2.bias', 'backbone.layers.2.blocks.8.mlp.fc1.weight', 'backbone.layers.2.blocks.8.mlp.fc1.bias', 'backbone.layers.2.blocks.8.mlp.fc2.weight', 'backbone.layers.2.blocks.8.mlp.fc2.bias', 'backbone.layers.2.blocks.9.norm1.weight', 'backbone.layers.2.blocks.9.norm1.bias', 'backbone.layers.2.blocks.9.attn.relative_position_bias_table', 'backbone.layers.2.blocks.9.attn.relative_position_index', 'backbone.layers.2.blocks.9.attn.qkv.weight', 'backbone.layers.2.blocks.9.attn.qkv.bias', 'backbone.layers.2.blocks.9.attn.proj.weight', 'backbone.layers.2.blocks.9.attn.proj.bias', 'backbone.layers.2.blocks.9.norm2.weight', 'backbone.layers.2.blocks.9.norm2.bias', 'backbone.layers.2.blocks.9.mlp.fc1.weight', 'backbone.layers.2.blocks.9.mlp.fc1.bias', 'backbone.layers.2.blocks.9.mlp.fc2.weight', 'backbone.layers.2.blocks.9.mlp.fc2.bias', 'backbone.layers.2.blocks.10.norm1.weight', 'backbone.layers.2.blocks.10.norm1.bias', 'backbone.layers.2.blocks.10.attn.relative_position_bias_table', 'backbone.layers.2.blocks.10.attn.relative_position_index', 'backbone.layers.2.blocks.10.attn.qkv.weight', 'backbone.layers.2.blocks.10.attn.qkv.bias', 'backbone.layers.2.blocks.10.attn.proj.weight', 'backbone.layers.2.blocks.10.attn.proj.bias', 'backbone.layers.2.blocks.10.norm2.weight', 'backbone.layers.2.blocks.10.norm2.bias', 'backbone.layers.2.blocks.10.mlp.fc1.weight', 'backbone.layers.2.blocks.10.mlp.fc1.bias', 'backbone.layers.2.blocks.10.mlp.fc2.weight', 'backbone.layers.2.blocks.10.mlp.fc2.bias', 'backbone.layers.2.blocks.11.norm1.weight', 'backbone.layers.2.blocks.11.norm1.bias', 'backbone.layers.2.blocks.11.attn.relative_position_bias_table', 'backbone.layers.2.blocks.11.attn.relative_position_index', 'backbone.layers.2.blocks.11.attn.qkv.weight', 'backbone.layers.2.blocks.11.attn.qkv.bias', 'backbone.layers.2.blocks.11.attn.proj.weight', 'backbone.layers.2.blocks.11.attn.proj.bias', 'backbone.layers.2.blocks.11.norm2.weight', 'backbone.layers.2.blocks.11.norm2.bias', 'backbone.layers.2.blocks.11.mlp.fc1.weight', 'backbone.layers.2.blocks.11.mlp.fc1.bias', 'backbone.layers.2.blocks.11.mlp.fc2.weight', 'backbone.layers.2.blocks.11.mlp.fc2.bias', 'backbone.layers.2.blocks.12.norm1.weight', 'backbone.layers.2.blocks.12.norm1.bias', 'backbone.layers.2.blocks.12.attn.relative_position_bias_table', 'backbone.layers.2.blocks.12.attn.relative_position_index', 'backbone.layers.2.blocks.12.attn.qkv.weight', 'backbone.layers.2.blocks.12.attn.qkv.bias', 'backbone.layers.2.blocks.12.attn.proj.weight', 'backbone.layers.2.blocks.12.attn.proj.bias', 'backbone.layers.2.blocks.12.norm2.weight', 'backbone.layers.2.blocks.12.norm2.bias', 'backbone.layers.2.blocks.12.mlp.fc1.weight', 'backbone.layers.2.blocks.12.mlp.fc1.bias', 'backbone.layers.2.blocks.12.mlp.fc2.weight', 'backbone.layers.2.blocks.12.mlp.fc2.bias', 'backbone.layers.2.blocks.13.norm1.weight', 'backbone.layers.2.blocks.13.norm1.bias', 'backbone.layers.2.blocks.13.attn.relative_position_bias_table', 'backbone.layers.2.blocks.13.attn.relative_position_index', 'backbone.layers.2.blocks.13.attn.qkv.weight', 'backbone.layers.2.blocks.13.attn.qkv.bias', 'backbone.layers.2.blocks.13.attn.proj.weight', 'backbone.layers.2.blocks.13.attn.proj.bias', 'backbone.layers.2.blocks.13.norm2.weight', 'backbone.layers.2.blocks.13.norm2.bias', 'backbone.layers.2.blocks.13.mlp.fc1.weight', 'backbone.layers.2.blocks.13.mlp.fc1.bias', 'backbone.layers.2.blocks.13.mlp.fc2.weight', 'backbone.layers.2.blocks.13.mlp.fc2.bias', 'backbone.layers.2.blocks.14.norm1.weight', 'backbone.layers.2.blocks.14.norm1.bias', 'backbone.layers.2.blocks.14.attn.relative_position_bias_table', 'backbone.layers.2.blocks.14.attn.relative_position_index', 'backbone.layers.2.blocks.14.attn.qkv.weight', 'backbone.layers.2.blocks.14.attn.qkv.bias', 'backbone.layers.2.blocks.14.attn.proj.weight', 'backbone.layers.2.blocks.14.attn.proj.bias', 'backbone.layers.2.blocks.14.norm2.weight', 'backbone.layers.2.blocks.14.norm2.bias', 'backbone.layers.2.blocks.14.mlp.fc1.weight', 'backbone.layers.2.blocks.14.mlp.fc1.bias', 'backbone.layers.2.blocks.14.mlp.fc2.weight', 'backbone.layers.2.blocks.14.mlp.fc2.bias', 'backbone.layers.2.blocks.15.norm1.weight', 'backbone.layers.2.blocks.15.norm1.bias', 'backbone.layers.2.blocks.15.attn.relative_position_bias_table', 'backbone.layers.2.blocks.15.attn.relative_position_index', 'backbone.layers.2.blocks.15.attn.qkv.weight', 'backbone.layers.2.blocks.15.attn.qkv.bias', 'backbone.layers.2.blocks.15.attn.proj.weight', 'backbone.layers.2.blocks.15.attn.proj.bias', 'backbone.layers.2.blocks.15.norm2.weight', 'backbone.layers.2.blocks.15.norm2.bias', 'backbone.layers.2.blocks.15.mlp.fc1.weight', 'backbone.layers.2.blocks.15.mlp.fc1.bias', 'backbone.layers.2.blocks.15.mlp.fc2.weight', 'backbone.layers.2.blocks.15.mlp.fc2.bias', 'backbone.layers.2.blocks.16.norm1.weight', 'backbone.layers.2.blocks.16.norm1.bias', 'backbone.layers.2.blocks.16.attn.relative_position_bias_table', 'backbone.layers.2.blocks.16.attn.relative_position_index', 'backbone.layers.2.blocks.16.attn.qkv.weight', 'backbone.layers.2.blocks.16.attn.qkv.bias', 'backbone.layers.2.blocks.16.attn.proj.weight', 'backbone.layers.2.blocks.16.attn.proj.bias', 'backbone.layers.2.blocks.16.norm2.weight', 'backbone.layers.2.blocks.16.norm2.bias', 'backbone.layers.2.blocks.16.mlp.fc1.weight', 'backbone.layers.2.blocks.16.mlp.fc1.bias', 'backbone.layers.2.blocks.16.mlp.fc2.weight', 'backbone.layers.2.blocks.16.mlp.fc2.bias', 'backbone.layers.2.blocks.17.norm1.weight', 'backbone.layers.2.blocks.17.norm1.bias', 'backbone.layers.2.blocks.17.attn.relative_position_bias_table', 'backbone.layers.2.blocks.17.attn.relative_position_index', 'backbone.layers.2.blocks.17.attn.qkv.weight', 'backbone.layers.2.blocks.17.attn.qkv.bias', 'backbone.layers.2.blocks.17.attn.proj.weight', 'backbone.layers.2.blocks.17.attn.proj.bias', 'backbone.layers.2.blocks.17.norm2.weight', 'backbone.layers.2.blocks.17.norm2.bias', 'backbone.layers.2.blocks.17.mlp.fc1.weight', 'backbone.layers.2.blocks.17.mlp.fc1.bias', 'backbone.layers.2.blocks.17.mlp.fc2.weight', 'backbone.layers.2.blocks.17.mlp.fc2.bias', 'backbone.layers.2.downsample.reduction.weight', 'backbone.layers.2.downsample.norm.weight', 'backbone.layers.2.downsample.norm.bias', 'backbone.layers.3.blocks.0.norm1.weight', 'backbone.layers.3.blocks.0.norm1.bias', 'backbone.layers.3.blocks.0.attn.relative_position_bias_table', 'backbone.layers.3.blocks.0.attn.relative_position_index', 'backbone.layers.3.blocks.0.attn.qkv.weight', 'backbone.layers.3.blocks.0.attn.qkv.bias', 'backbone.layers.3.blocks.0.attn.proj.weight', 'backbone.layers.3.blocks.0.attn.proj.bias', 'backbone.layers.3.blocks.0.norm2.weight', 'backbone.layers.3.blocks.0.norm2.bias', 'backbone.layers.3.blocks.0.mlp.fc1.weight', 'backbone.layers.3.blocks.0.mlp.fc1.bias', 'backbone.layers.3.blocks.0.mlp.fc2.weight', 'backbone.layers.3.blocks.0.mlp.fc2.bias', 'backbone.layers.3.blocks.1.norm1.weight', 'backbone.layers.3.blocks.1.norm1.bias', 'backbone.layers.3.blocks.1.attn.relative_position_bias_table', 'backbone.layers.3.blocks.1.attn.relative_position_index', 'backbone.layers.3.blocks.1.attn.qkv.weight', 'backbone.layers.3.blocks.1.attn.qkv.bias', 'backbone.layers.3.blocks.1.attn.proj.weight', 'backbone.layers.3.blocks.1.attn.proj.bias', 'backbone.layers.3.blocks.1.norm2.weight', 'backbone.layers.3.blocks.1.norm2.bias', 'backbone.layers.3.blocks.1.mlp.fc1.weight', 'backbone.layers.3.blocks.1.mlp.fc1.bias', 'backbone.layers.3.blocks.1.mlp.fc2.weight', 'backbone.layers.3.blocks.1.mlp.fc2.bias', 'cls_head.fc_cls.weight', 'cls_head.fc_cls.bias', 'backbone.norm3.weight', 'backbone.norm3.bias']\n"
     ]
    }
   ],
   "source": [
    "exract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\belongTH\\anaconda3\\envs\\testpyt\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['patch_embed.proj.weight', 'patch_embed.proj.bias', 'patch_embed.norm.weight', 'patch_embed.norm.bias', 'swinaclayer1.blocks.0.norm1.weight', 'swinaclayer1.blocks.0.norm1.bias', 'swinaclayer1.blocks.0.attn.relative_position_bias_table', 'swinaclayer1.blocks.0.attn.relative_position_index', 'swinaclayer1.blocks.0.attn.qkv.weight', 'swinaclayer1.blocks.0.attn.qkv.bias', 'swinaclayer1.blocks.0.attn.proj.weight', 'swinaclayer1.blocks.0.attn.proj.bias', 'swinaclayer1.blocks.0.norm2.weight', 'swinaclayer1.blocks.0.norm2.bias', 'swinaclayer1.blocks.0.mlp.fc1.weight', 'swinaclayer1.blocks.0.mlp.fc1.bias', 'swinaclayer1.blocks.0.mlp.fc2.weight', 'swinaclayer1.blocks.0.mlp.fc2.bias', 'swinaclayer1.blocks.1.norm1.weight', 'swinaclayer1.blocks.1.norm1.bias', 'swinaclayer1.blocks.1.attn.relative_position_bias_table', 'swinaclayer1.blocks.1.attn.relative_position_index', 'swinaclayer1.blocks.1.attn.qkv.weight', 'swinaclayer1.blocks.1.attn.qkv.bias', 'swinaclayer1.blocks.1.attn.proj.weight', 'swinaclayer1.blocks.1.attn.proj.bias', 'swinaclayer1.blocks.1.norm2.weight', 'swinaclayer1.blocks.1.norm2.bias', 'swinaclayer1.blocks.1.mlp.fc1.weight', 'swinaclayer1.blocks.1.mlp.fc1.bias', 'swinaclayer1.blocks.1.mlp.fc2.weight', 'swinaclayer1.blocks.1.mlp.fc2.bias', 'swinaclayer1.downsample.reduction.weight', 'swinaclayer1.downsample.norm.weight', 'swinaclayer1.downsample.norm.bias', 'swinaclayer2.blocks.0.norm1.weight', 'swinaclayer2.blocks.0.norm1.bias', 'swinaclayer2.blocks.0.attn.relative_position_bias_table', 'swinaclayer2.blocks.0.attn.relative_position_index', 'swinaclayer2.blocks.0.attn.qkv.weight', 'swinaclayer2.blocks.0.attn.qkv.bias', 'swinaclayer2.blocks.0.attn.proj.weight', 'swinaclayer2.blocks.0.attn.proj.bias', 'swinaclayer2.blocks.0.norm2.weight', 'swinaclayer2.blocks.0.norm2.bias', 'swinaclayer2.blocks.0.mlp.fc1.weight', 'swinaclayer2.blocks.0.mlp.fc1.bias', 'swinaclayer2.blocks.0.mlp.fc2.weight', 'swinaclayer2.blocks.0.mlp.fc2.bias', 'swinaclayer2.blocks.1.norm1.weight', 'swinaclayer2.blocks.1.norm1.bias', 'swinaclayer2.blocks.1.attn.relative_position_bias_table', 'swinaclayer2.blocks.1.attn.relative_position_index', 'swinaclayer2.blocks.1.attn.qkv.weight', 'swinaclayer2.blocks.1.attn.qkv.bias', 'swinaclayer2.blocks.1.attn.proj.weight', 'swinaclayer2.blocks.1.attn.proj.bias', 'swinaclayer2.blocks.1.norm2.weight', 'swinaclayer2.blocks.1.norm2.bias', 'swinaclayer2.blocks.1.mlp.fc1.weight', 'swinaclayer2.blocks.1.mlp.fc1.bias', 'swinaclayer2.blocks.1.mlp.fc2.weight', 'swinaclayer2.blocks.1.mlp.fc2.bias', 'swinaclayer2.downsample.reduction.weight', 'swinaclayer2.downsample.norm.weight', 'swinaclayer2.downsample.norm.bias', 'swinaclayer3.blocks.0.norm1.weight', 'swinaclayer3.blocks.0.norm1.bias', 'swinaclayer3.blocks.0.attn.relative_position_bias_table', 'swinaclayer3.blocks.0.attn.relative_position_index', 'swinaclayer3.blocks.0.attn.qkv.weight', 'swinaclayer3.blocks.0.attn.qkv.bias', 'swinaclayer3.blocks.0.attn.proj.weight', 'swinaclayer3.blocks.0.attn.proj.bias', 'swinaclayer3.blocks.0.norm2.weight', 'swinaclayer3.blocks.0.norm2.bias', 'swinaclayer3.blocks.0.mlp.fc1.weight', 'swinaclayer3.blocks.0.mlp.fc1.bias', 'swinaclayer3.blocks.0.mlp.fc2.weight', 'swinaclayer3.blocks.0.mlp.fc2.bias', 'swinaclayer3.blocks.1.norm1.weight', 'swinaclayer3.blocks.1.norm1.bias', 'swinaclayer3.blocks.1.attn.relative_position_bias_table', 'swinaclayer3.blocks.1.attn.relative_position_index', 'swinaclayer3.blocks.1.attn.qkv.weight', 'swinaclayer3.blocks.1.attn.qkv.bias', 'swinaclayer3.blocks.1.attn.proj.weight', 'swinaclayer3.blocks.1.attn.proj.bias', 'swinaclayer3.blocks.1.norm2.weight', 'swinaclayer3.blocks.1.norm2.bias', 'swinaclayer3.blocks.1.mlp.fc1.weight', 'swinaclayer3.blocks.1.mlp.fc1.bias', 'swinaclayer3.blocks.1.mlp.fc2.weight', 'swinaclayer3.blocks.1.mlp.fc2.bias', 'swinaclayer3.blocks.2.norm1.weight', 'swinaclayer3.blocks.2.norm1.bias', 'swinaclayer3.blocks.2.attn.relative_position_bias_table', 'swinaclayer3.blocks.2.attn.relative_position_index', 'swinaclayer3.blocks.2.attn.qkv.weight', 'swinaclayer3.blocks.2.attn.qkv.bias', 'swinaclayer3.blocks.2.attn.proj.weight', 'swinaclayer3.blocks.2.attn.proj.bias', 'swinaclayer3.blocks.2.norm2.weight', 'swinaclayer3.blocks.2.norm2.bias', 'swinaclayer3.blocks.2.mlp.fc1.weight', 'swinaclayer3.blocks.2.mlp.fc1.bias', 'swinaclayer3.blocks.2.mlp.fc2.weight', 'swinaclayer3.blocks.2.mlp.fc2.bias', 'swinaclayer3.blocks.3.norm1.weight', 'swinaclayer3.blocks.3.norm1.bias', 'swinaclayer3.blocks.3.attn.relative_position_bias_table', 'swinaclayer3.blocks.3.attn.relative_position_index', 'swinaclayer3.blocks.3.attn.qkv.weight', 'swinaclayer3.blocks.3.attn.qkv.bias', 'swinaclayer3.blocks.3.attn.proj.weight', 'swinaclayer3.blocks.3.attn.proj.bias', 'swinaclayer3.blocks.3.norm2.weight', 'swinaclayer3.blocks.3.norm2.bias', 'swinaclayer3.blocks.3.mlp.fc1.weight', 'swinaclayer3.blocks.3.mlp.fc1.bias', 'swinaclayer3.blocks.3.mlp.fc2.weight', 'swinaclayer3.blocks.3.mlp.fc2.bias', 'swinaclayer3.blocks.4.norm1.weight', 'swinaclayer3.blocks.4.norm1.bias', 'swinaclayer3.blocks.4.attn.relative_position_bias_table', 'swinaclayer3.blocks.4.attn.relative_position_index', 'swinaclayer3.blocks.4.attn.qkv.weight', 'swinaclayer3.blocks.4.attn.qkv.bias', 'swinaclayer3.blocks.4.attn.proj.weight', 'swinaclayer3.blocks.4.attn.proj.bias', 'swinaclayer3.blocks.4.norm2.weight', 'swinaclayer3.blocks.4.norm2.bias', 'swinaclayer3.blocks.4.mlp.fc1.weight', 'swinaclayer3.blocks.4.mlp.fc1.bias', 'swinaclayer3.blocks.4.mlp.fc2.weight', 'swinaclayer3.blocks.4.mlp.fc2.bias', 'swinaclayer3.blocks.5.norm1.weight', 'swinaclayer3.blocks.5.norm1.bias', 'swinaclayer3.blocks.5.attn.relative_position_bias_table', 'swinaclayer3.blocks.5.attn.relative_position_index', 'swinaclayer3.blocks.5.attn.qkv.weight', 'swinaclayer3.blocks.5.attn.qkv.bias', 'swinaclayer3.blocks.5.attn.proj.weight', 'swinaclayer3.blocks.5.attn.proj.bias', 'swinaclayer3.blocks.5.norm2.weight', 'swinaclayer3.blocks.5.norm2.bias', 'swinaclayer3.blocks.5.mlp.fc1.weight', 'swinaclayer3.blocks.5.mlp.fc1.bias', 'swinaclayer3.blocks.5.mlp.fc2.weight', 'swinaclayer3.blocks.5.mlp.fc2.bias', 'swinaclayer3.blocks.6.norm1.weight', 'swinaclayer3.blocks.6.norm1.bias', 'swinaclayer3.blocks.6.attn.relative_position_bias_table', 'swinaclayer3.blocks.6.attn.relative_position_index', 'swinaclayer3.blocks.6.attn.qkv.weight', 'swinaclayer3.blocks.6.attn.qkv.bias', 'swinaclayer3.blocks.6.attn.proj.weight', 'swinaclayer3.blocks.6.attn.proj.bias', 'swinaclayer3.blocks.6.norm2.weight', 'swinaclayer3.blocks.6.norm2.bias', 'swinaclayer3.blocks.6.mlp.fc1.weight', 'swinaclayer3.blocks.6.mlp.fc1.bias', 'swinaclayer3.blocks.6.mlp.fc2.weight', 'swinaclayer3.blocks.6.mlp.fc2.bias', 'swinaclayer3.blocks.7.norm1.weight', 'swinaclayer3.blocks.7.norm1.bias', 'swinaclayer3.blocks.7.attn.relative_position_bias_table', 'swinaclayer3.blocks.7.attn.relative_position_index', 'swinaclayer3.blocks.7.attn.qkv.weight', 'swinaclayer3.blocks.7.attn.qkv.bias', 'swinaclayer3.blocks.7.attn.proj.weight', 'swinaclayer3.blocks.7.attn.proj.bias', 'swinaclayer3.blocks.7.norm2.weight', 'swinaclayer3.blocks.7.norm2.bias', 'swinaclayer3.blocks.7.mlp.fc1.weight', 'swinaclayer3.blocks.7.mlp.fc1.bias', 'swinaclayer3.blocks.7.mlp.fc2.weight', 'swinaclayer3.blocks.7.mlp.fc2.bias', 'swinaclayer3.blocks.8.norm1.weight', 'swinaclayer3.blocks.8.norm1.bias', 'swinaclayer3.blocks.8.attn.relative_position_bias_table', 'swinaclayer3.blocks.8.attn.relative_position_index', 'swinaclayer3.blocks.8.attn.qkv.weight', 'swinaclayer3.blocks.8.attn.qkv.bias', 'swinaclayer3.blocks.8.attn.proj.weight', 'swinaclayer3.blocks.8.attn.proj.bias', 'swinaclayer3.blocks.8.norm2.weight', 'swinaclayer3.blocks.8.norm2.bias', 'swinaclayer3.blocks.8.mlp.fc1.weight', 'swinaclayer3.blocks.8.mlp.fc1.bias', 'swinaclayer3.blocks.8.mlp.fc2.weight', 'swinaclayer3.blocks.8.mlp.fc2.bias', 'swinaclayer3.blocks.9.norm1.weight', 'swinaclayer3.blocks.9.norm1.bias', 'swinaclayer3.blocks.9.attn.relative_position_bias_table', 'swinaclayer3.blocks.9.attn.relative_position_index', 'swinaclayer3.blocks.9.attn.qkv.weight', 'swinaclayer3.blocks.9.attn.qkv.bias', 'swinaclayer3.blocks.9.attn.proj.weight', 'swinaclayer3.blocks.9.attn.proj.bias', 'swinaclayer3.blocks.9.norm2.weight', 'swinaclayer3.blocks.9.norm2.bias', 'swinaclayer3.blocks.9.mlp.fc1.weight', 'swinaclayer3.blocks.9.mlp.fc1.bias', 'swinaclayer3.blocks.9.mlp.fc2.weight', 'swinaclayer3.blocks.9.mlp.fc2.bias', 'swinaclayer3.blocks.10.norm1.weight', 'swinaclayer3.blocks.10.norm1.bias', 'swinaclayer3.blocks.10.attn.relative_position_bias_table', 'swinaclayer3.blocks.10.attn.relative_position_index', 'swinaclayer3.blocks.10.attn.qkv.weight', 'swinaclayer3.blocks.10.attn.qkv.bias', 'swinaclayer3.blocks.10.attn.proj.weight', 'swinaclayer3.blocks.10.attn.proj.bias', 'swinaclayer3.blocks.10.norm2.weight', 'swinaclayer3.blocks.10.norm2.bias', 'swinaclayer3.blocks.10.mlp.fc1.weight', 'swinaclayer3.blocks.10.mlp.fc1.bias', 'swinaclayer3.blocks.10.mlp.fc2.weight', 'swinaclayer3.blocks.10.mlp.fc2.bias', 'swinaclayer3.blocks.11.norm1.weight', 'swinaclayer3.blocks.11.norm1.bias', 'swinaclayer3.blocks.11.attn.relative_position_bias_table', 'swinaclayer3.blocks.11.attn.relative_position_index', 'swinaclayer3.blocks.11.attn.qkv.weight', 'swinaclayer3.blocks.11.attn.qkv.bias', 'swinaclayer3.blocks.11.attn.proj.weight', 'swinaclayer3.blocks.11.attn.proj.bias', 'swinaclayer3.blocks.11.norm2.weight', 'swinaclayer3.blocks.11.norm2.bias', 'swinaclayer3.blocks.11.mlp.fc1.weight', 'swinaclayer3.blocks.11.mlp.fc1.bias', 'swinaclayer3.blocks.11.mlp.fc2.weight', 'swinaclayer3.blocks.11.mlp.fc2.bias', 'swinaclayer3.blocks.12.norm1.weight', 'swinaclayer3.blocks.12.norm1.bias', 'swinaclayer3.blocks.12.attn.relative_position_bias_table', 'swinaclayer3.blocks.12.attn.relative_position_index', 'swinaclayer3.blocks.12.attn.qkv.weight', 'swinaclayer3.blocks.12.attn.qkv.bias', 'swinaclayer3.blocks.12.attn.proj.weight', 'swinaclayer3.blocks.12.attn.proj.bias', 'swinaclayer3.blocks.12.norm2.weight', 'swinaclayer3.blocks.12.norm2.bias', 'swinaclayer3.blocks.12.mlp.fc1.weight', 'swinaclayer3.blocks.12.mlp.fc1.bias', 'swinaclayer3.blocks.12.mlp.fc2.weight', 'swinaclayer3.blocks.12.mlp.fc2.bias', 'swinaclayer3.blocks.13.norm1.weight', 'swinaclayer3.blocks.13.norm1.bias', 'swinaclayer3.blocks.13.attn.relative_position_bias_table', 'swinaclayer3.blocks.13.attn.relative_position_index', 'swinaclayer3.blocks.13.attn.qkv.weight', 'swinaclayer3.blocks.13.attn.qkv.bias', 'swinaclayer3.blocks.13.attn.proj.weight', 'swinaclayer3.blocks.13.attn.proj.bias', 'swinaclayer3.blocks.13.norm2.weight', 'swinaclayer3.blocks.13.norm2.bias', 'swinaclayer3.blocks.13.mlp.fc1.weight', 'swinaclayer3.blocks.13.mlp.fc1.bias', 'swinaclayer3.blocks.13.mlp.fc2.weight', 'swinaclayer3.blocks.13.mlp.fc2.bias', 'swinaclayer3.blocks.14.norm1.weight', 'swinaclayer3.blocks.14.norm1.bias', 'swinaclayer3.blocks.14.attn.relative_position_bias_table', 'swinaclayer3.blocks.14.attn.relative_position_index', 'swinaclayer3.blocks.14.attn.qkv.weight', 'swinaclayer3.blocks.14.attn.qkv.bias', 'swinaclayer3.blocks.14.attn.proj.weight', 'swinaclayer3.blocks.14.attn.proj.bias', 'swinaclayer3.blocks.14.norm2.weight', 'swinaclayer3.blocks.14.norm2.bias', 'swinaclayer3.blocks.14.mlp.fc1.weight', 'swinaclayer3.blocks.14.mlp.fc1.bias', 'swinaclayer3.blocks.14.mlp.fc2.weight', 'swinaclayer3.blocks.14.mlp.fc2.bias', 'swinaclayer3.blocks.15.norm1.weight', 'swinaclayer3.blocks.15.norm1.bias', 'swinaclayer3.blocks.15.attn.relative_position_bias_table', 'swinaclayer3.blocks.15.attn.relative_position_index', 'swinaclayer3.blocks.15.attn.qkv.weight', 'swinaclayer3.blocks.15.attn.qkv.bias', 'swinaclayer3.blocks.15.attn.proj.weight', 'swinaclayer3.blocks.15.attn.proj.bias', 'swinaclayer3.blocks.15.norm2.weight', 'swinaclayer3.blocks.15.norm2.bias', 'swinaclayer3.blocks.15.mlp.fc1.weight', 'swinaclayer3.blocks.15.mlp.fc1.bias', 'swinaclayer3.blocks.15.mlp.fc2.weight', 'swinaclayer3.blocks.15.mlp.fc2.bias', 'swinaclayer3.blocks.16.norm1.weight', 'swinaclayer3.blocks.16.norm1.bias', 'swinaclayer3.blocks.16.attn.relative_position_bias_table', 'swinaclayer3.blocks.16.attn.relative_position_index', 'swinaclayer3.blocks.16.attn.qkv.weight', 'swinaclayer3.blocks.16.attn.qkv.bias', 'swinaclayer3.blocks.16.attn.proj.weight', 'swinaclayer3.blocks.16.attn.proj.bias', 'swinaclayer3.blocks.16.norm2.weight', 'swinaclayer3.blocks.16.norm2.bias', 'swinaclayer3.blocks.16.mlp.fc1.weight', 'swinaclayer3.blocks.16.mlp.fc1.bias', 'swinaclayer3.blocks.16.mlp.fc2.weight', 'swinaclayer3.blocks.16.mlp.fc2.bias', 'swinaclayer3.blocks.17.norm1.weight', 'swinaclayer3.blocks.17.norm1.bias', 'swinaclayer3.blocks.17.attn.relative_position_bias_table', 'swinaclayer3.blocks.17.attn.relative_position_index', 'swinaclayer3.blocks.17.attn.qkv.weight', 'swinaclayer3.blocks.17.attn.qkv.bias', 'swinaclayer3.blocks.17.attn.proj.weight', 'swinaclayer3.blocks.17.attn.proj.bias', 'swinaclayer3.blocks.17.norm2.weight', 'swinaclayer3.blocks.17.norm2.bias', 'swinaclayer3.blocks.17.mlp.fc1.weight', 'swinaclayer3.blocks.17.mlp.fc1.bias', 'swinaclayer3.blocks.17.mlp.fc2.weight', 'swinaclayer3.blocks.17.mlp.fc2.bias', 'swinaclayer3.downsample.reduction.weight', 'swinaclayer3.downsample.norm.weight', 'swinaclayer3.downsample.norm.bias', 'swinaclayer4.blocks.0.norm1.weight', 'swinaclayer4.blocks.0.norm1.bias', 'swinaclayer4.blocks.0.attn.relative_position_bias_table', 'swinaclayer4.blocks.0.attn.relative_position_index', 'swinaclayer4.blocks.0.attn.qkv.weight', 'swinaclayer4.blocks.0.attn.qkv.bias', 'swinaclayer4.blocks.0.attn.proj.weight', 'swinaclayer4.blocks.0.attn.proj.bias', 'swinaclayer4.blocks.0.norm2.weight', 'swinaclayer4.blocks.0.norm2.bias', 'swinaclayer4.blocks.0.mlp.fc1.weight', 'swinaclayer4.blocks.0.mlp.fc1.bias', 'swinaclayer4.blocks.0.mlp.fc2.weight', 'swinaclayer4.blocks.0.mlp.fc2.bias', 'swinaclayer4.blocks.1.norm1.weight', 'swinaclayer4.blocks.1.norm1.bias', 'swinaclayer4.blocks.1.attn.relative_position_bias_table', 'swinaclayer4.blocks.1.attn.relative_position_index', 'swinaclayer4.blocks.1.attn.qkv.weight', 'swinaclayer4.blocks.1.attn.qkv.bias', 'swinaclayer4.blocks.1.attn.proj.weight', 'swinaclayer4.blocks.1.attn.proj.bias', 'swinaclayer4.blocks.1.norm2.weight', 'swinaclayer4.blocks.1.norm2.bias', 'swinaclayer4.blocks.1.mlp.fc1.weight', 'swinaclayer4.blocks.1.mlp.fc1.bias', 'swinaclayer4.blocks.1.mlp.fc2.weight', 'swinaclayer4.blocks.1.mlp.fc2.bias', 'norm3.weight', 'norm3.bias', 'upcov1.weight', 'upcov2.weight', 'upcov2.bias', 'upcov3.weight', 'upcov3.bias', 'upcov4.weight', 'layer1.0.conv1.weight', 'layer1.0.cgru.recurrent_conv_gates.weight', 'layer1.0.cgru.recurrent_conv_gates.bias', 'layer1.0.cgru.recurrent_conv_out.weight', 'layer1.0.cgru.recurrent_conv_out.bias', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.bn3.bias', 'layer1.0.bn3.running_mean', 'layer1.0.bn3.running_var', 'layer1.0.bn3.num_batches_tracked', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.cgru.recurrent_conv_gates.weight', 'layer1.1.cgru.recurrent_conv_gates.bias', 'layer1.1.cgru.recurrent_conv_out.weight', 'layer1.1.cgru.recurrent_conv_out.bias', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.1.bn3.bias', 'layer1.1.bn3.running_mean', 'layer1.1.bn3.running_var', 'layer1.1.bn3.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.cgru.recurrent_conv_gates.weight', 'layer1.2.cgru.recurrent_conv_gates.bias', 'layer1.2.cgru.recurrent_conv_out.weight', 'layer1.2.cgru.recurrent_conv_out.bias', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer1.2.bn3.bias', 'layer1.2.bn3.running_mean', 'layer1.2.bn3.running_var', 'layer1.2.bn3.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.cgru.recurrent_conv_gates.weight', 'layer2.0.cgru.recurrent_conv_gates.bias', 'layer2.0.cgru.recurrent_conv_out.weight', 'layer2.0.cgru.recurrent_conv_out.bias', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn3.bias', 'layer2.0.bn3.running_mean', 'layer2.0.bn3.running_var', 'layer2.0.bn3.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.1.bn3.bias', 'layer2.1.bn3.running_mean', 'layer2.1.bn3.running_var', 'layer2.1.bn3.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.cgru.recurrent_conv_gates.weight', 'layer2.2.cgru.recurrent_conv_gates.bias', 'layer2.2.cgru.recurrent_conv_out.weight', 'layer2.2.cgru.recurrent_conv_out.bias', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.2.bn3.bias', 'layer2.2.bn3.running_mean', 'layer2.2.bn3.running_var', 'layer2.2.bn3.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn3.bias', 'layer2.3.bn3.running_mean', 'layer2.3.bn3.running_var', 'layer2.3.bn3.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.cgru.recurrent_conv_gates.weight', 'layer3.0.cgru.recurrent_conv_gates.bias', 'layer3.0.cgru.recurrent_conv_out.weight', 'layer3.0.cgru.recurrent_conv_out.bias', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.bn3.bias', 'layer3.0.bn3.running_mean', 'layer3.0.bn3.running_var', 'layer3.0.bn3.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.1.bn3.bias', 'layer3.1.bn3.running_mean', 'layer3.1.bn3.running_var', 'layer3.1.bn3.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.cgru.recurrent_conv_gates.weight', 'layer3.2.cgru.recurrent_conv_gates.bias', 'layer3.2.cgru.recurrent_conv_out.weight', 'layer3.2.cgru.recurrent_conv_out.bias', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.2.bn3.bias', 'layer3.2.bn3.running_mean', 'layer3.2.bn3.running_var', 'layer3.2.bn3.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.3.bn3.bias', 'layer3.3.bn3.running_mean', 'layer3.3.bn3.running_var', 'layer3.3.bn3.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.cgru.recurrent_conv_gates.weight', 'layer3.4.cgru.recurrent_conv_gates.bias', 'layer3.4.cgru.recurrent_conv_out.weight', 'layer3.4.cgru.recurrent_conv_out.bias', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.4.bn3.bias', 'layer3.4.bn3.running_mean', 'layer3.4.bn3.running_var', 'layer3.4.bn3.num_batches_tracked', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer3.5.bn3.bias', 'layer3.5.bn3.running_mean', 'layer3.5.bn3.running_var', 'layer3.5.bn3.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.cgru.recurrent_conv_gates.weight', 'layer4.0.cgru.recurrent_conv_gates.bias', 'layer4.0.cgru.recurrent_conv_out.weight', 'layer4.0.cgru.recurrent_conv_out.bias', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.bn3.bias', 'layer4.0.bn3.running_mean', 'layer4.0.bn3.running_var', 'layer4.0.bn3.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.cgru.recurrent_conv_gates.weight', 'layer4.1.cgru.recurrent_conv_gates.bias', 'layer4.1.cgru.recurrent_conv_out.weight', 'layer4.1.cgru.recurrent_conv_out.bias', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn3.bias', 'layer4.1.bn3.running_mean', 'layer4.1.bn3.running_var', 'layer4.1.bn3.num_batches_tracked', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias', 'layer4.2.bn3.running_mean', 'layer4.2.bn3.running_var', 'layer4.2.bn3.num_batches_tracked', 'conv6.weight', 'conv7.weight', 'lateral_layer1.weight', 'lateral_layer2.weight', 'lateral_layer3.weight', 'corr_layer1.weight', 'corr_layer2.weight', 'corr_layer3.weight'])\n"
     ]
    }
   ],
   "source": [
    "model=ResNetFPN(BottleneckRCGRU,args)\n",
    "\n",
    "model_stat_dict = model.state_dict()\n",
    "print(model_stat_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['upcov1.weight', 'upcov2.weight', 'upcov2.bias', 'upcov3.weight', 'upcov3.bias', 'upcov4.weight'], unexpected_keys=['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layers.0.blocks.0.norm1.weight', 'layers.0.blocks.0.norm1.bias', 'layers.0.blocks.0.attn.relative_position_bias_table', 'layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.attn.qkv.weight', 'layers.0.blocks.0.attn.qkv.bias', 'layers.0.blocks.0.attn.proj.weight', 'layers.0.blocks.0.attn.proj.bias', 'layers.0.blocks.0.norm2.weight', 'layers.0.blocks.0.norm2.bias', 'layers.0.blocks.0.mlp.fc1.weight', 'layers.0.blocks.0.mlp.fc1.bias', 'layers.0.blocks.0.mlp.fc2.weight', 'layers.0.blocks.0.mlp.fc2.bias', 'layers.0.blocks.1.norm1.weight', 'layers.0.blocks.1.norm1.bias', 'layers.0.blocks.1.attn.relative_position_bias_table', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.attn.qkv.weight', 'layers.0.blocks.1.attn.qkv.bias', 'layers.0.blocks.1.attn.proj.weight', 'layers.0.blocks.1.attn.proj.bias', 'layers.0.blocks.1.norm2.weight', 'layers.0.blocks.1.norm2.bias', 'layers.0.blocks.1.mlp.fc1.weight', 'layers.0.blocks.1.mlp.fc1.bias', 'layers.0.blocks.1.mlp.fc2.weight', 'layers.0.blocks.1.mlp.fc2.bias', 'layers.0.downsample.reduction.weight', 'layers.0.downsample.norm.weight', 'layers.0.downsample.norm.bias', 'layers.1.blocks.0.norm1.weight', 'layers.1.blocks.0.norm1.bias', 'layers.1.blocks.0.attn.relative_position_bias_table', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.qkv.weight', 'layers.1.blocks.0.attn.qkv.bias', 'layers.1.blocks.0.attn.proj.weight', 'layers.1.blocks.0.attn.proj.bias', 'layers.1.blocks.0.norm2.weight', 'layers.1.blocks.0.norm2.bias', 'layers.1.blocks.0.mlp.fc1.weight', 'layers.1.blocks.0.mlp.fc1.bias', 'layers.1.blocks.0.mlp.fc2.weight', 'layers.1.blocks.0.mlp.fc2.bias', 'layers.1.blocks.1.norm1.weight', 'layers.1.blocks.1.norm1.bias', 'layers.1.blocks.1.attn.relative_position_bias_table', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.attn.qkv.weight', 'layers.1.blocks.1.attn.qkv.bias', 'layers.1.blocks.1.attn.proj.weight', 'layers.1.blocks.1.attn.proj.bias', 'layers.1.blocks.1.norm2.weight', 'layers.1.blocks.1.norm2.bias', 'layers.1.blocks.1.mlp.fc1.weight', 'layers.1.blocks.1.mlp.fc1.bias', 'layers.1.blocks.1.mlp.fc2.weight', 'layers.1.blocks.1.mlp.fc2.bias', 'layers.1.downsample.reduction.weight', 'layers.1.downsample.norm.weight', 'layers.1.downsample.norm.bias', 'layers.2.blocks.0.norm1.weight', 'layers.2.blocks.0.norm1.bias', 'layers.2.blocks.0.attn.relative_position_bias_table', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.qkv.weight', 'layers.2.blocks.0.attn.qkv.bias', 'layers.2.blocks.0.attn.proj.weight', 'layers.2.blocks.0.attn.proj.bias', 'layers.2.blocks.0.norm2.weight', 'layers.2.blocks.0.norm2.bias', 'layers.2.blocks.0.mlp.fc1.weight', 'layers.2.blocks.0.mlp.fc1.bias', 'layers.2.blocks.0.mlp.fc2.weight', 'layers.2.blocks.0.mlp.fc2.bias', 'layers.2.blocks.1.norm1.weight', 'layers.2.blocks.1.norm1.bias', 'layers.2.blocks.1.attn.relative_position_bias_table', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.attn.qkv.weight', 'layers.2.blocks.1.attn.qkv.bias', 'layers.2.blocks.1.attn.proj.weight', 'layers.2.blocks.1.attn.proj.bias', 'layers.2.blocks.1.norm2.weight', 'layers.2.blocks.1.norm2.bias', 'layers.2.blocks.1.mlp.fc1.weight', 'layers.2.blocks.1.mlp.fc1.bias', 'layers.2.blocks.1.mlp.fc2.weight', 'layers.2.blocks.1.mlp.fc2.bias', 'layers.2.blocks.2.norm1.weight', 'layers.2.blocks.2.norm1.bias', 'layers.2.blocks.2.attn.relative_position_bias_table', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.attn.qkv.weight', 'layers.2.blocks.2.attn.qkv.bias', 'layers.2.blocks.2.attn.proj.weight', 'layers.2.blocks.2.attn.proj.bias', 'layers.2.blocks.2.norm2.weight', 'layers.2.blocks.2.norm2.bias', 'layers.2.blocks.2.mlp.fc1.weight', 'layers.2.blocks.2.mlp.fc1.bias', 'layers.2.blocks.2.mlp.fc2.weight', 'layers.2.blocks.2.mlp.fc2.bias', 'layers.2.blocks.3.norm1.weight', 'layers.2.blocks.3.norm1.bias', 'layers.2.blocks.3.attn.relative_position_bias_table', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.attn.qkv.weight', 'layers.2.blocks.3.attn.qkv.bias', 'layers.2.blocks.3.attn.proj.weight', 'layers.2.blocks.3.attn.proj.bias', 'layers.2.blocks.3.norm2.weight', 'layers.2.blocks.3.norm2.bias', 'layers.2.blocks.3.mlp.fc1.weight', 'layers.2.blocks.3.mlp.fc1.bias', 'layers.2.blocks.3.mlp.fc2.weight', 'layers.2.blocks.3.mlp.fc2.bias', 'layers.2.blocks.4.norm1.weight', 'layers.2.blocks.4.norm1.bias', 'layers.2.blocks.4.attn.relative_position_bias_table', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.attn.qkv.weight', 'layers.2.blocks.4.attn.qkv.bias', 'layers.2.blocks.4.attn.proj.weight', 'layers.2.blocks.4.attn.proj.bias', 'layers.2.blocks.4.norm2.weight', 'layers.2.blocks.4.norm2.bias', 'layers.2.blocks.4.mlp.fc1.weight', 'layers.2.blocks.4.mlp.fc1.bias', 'layers.2.blocks.4.mlp.fc2.weight', 'layers.2.blocks.4.mlp.fc2.bias', 'layers.2.blocks.5.norm1.weight', 'layers.2.blocks.5.norm1.bias', 'layers.2.blocks.5.attn.relative_position_bias_table', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.attn.qkv.weight', 'layers.2.blocks.5.attn.qkv.bias', 'layers.2.blocks.5.attn.proj.weight', 'layers.2.blocks.5.attn.proj.bias', 'layers.2.blocks.5.norm2.weight', 'layers.2.blocks.5.norm2.bias', 'layers.2.blocks.5.mlp.fc1.weight', 'layers.2.blocks.5.mlp.fc1.bias', 'layers.2.blocks.5.mlp.fc2.weight', 'layers.2.blocks.5.mlp.fc2.bias', 'layers.2.blocks.6.norm1.weight', 'layers.2.blocks.6.norm1.bias', 'layers.2.blocks.6.attn.relative_position_bias_table', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.6.attn.qkv.weight', 'layers.2.blocks.6.attn.qkv.bias', 'layers.2.blocks.6.attn.proj.weight', 'layers.2.blocks.6.attn.proj.bias', 'layers.2.blocks.6.norm2.weight', 'layers.2.blocks.6.norm2.bias', 'layers.2.blocks.6.mlp.fc1.weight', 'layers.2.blocks.6.mlp.fc1.bias', 'layers.2.blocks.6.mlp.fc2.weight', 'layers.2.blocks.6.mlp.fc2.bias', 'layers.2.blocks.7.norm1.weight', 'layers.2.blocks.7.norm1.bias', 'layers.2.blocks.7.attn.relative_position_bias_table', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.7.attn.qkv.weight', 'layers.2.blocks.7.attn.qkv.bias', 'layers.2.blocks.7.attn.proj.weight', 'layers.2.blocks.7.attn.proj.bias', 'layers.2.blocks.7.norm2.weight', 'layers.2.blocks.7.norm2.bias', 'layers.2.blocks.7.mlp.fc1.weight', 'layers.2.blocks.7.mlp.fc1.bias', 'layers.2.blocks.7.mlp.fc2.weight', 'layers.2.blocks.7.mlp.fc2.bias', 'layers.2.blocks.8.norm1.weight', 'layers.2.blocks.8.norm1.bias', 'layers.2.blocks.8.attn.relative_position_bias_table', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.8.attn.qkv.weight', 'layers.2.blocks.8.attn.qkv.bias', 'layers.2.blocks.8.attn.proj.weight', 'layers.2.blocks.8.attn.proj.bias', 'layers.2.blocks.8.norm2.weight', 'layers.2.blocks.8.norm2.bias', 'layers.2.blocks.8.mlp.fc1.weight', 'layers.2.blocks.8.mlp.fc1.bias', 'layers.2.blocks.8.mlp.fc2.weight', 'layers.2.blocks.8.mlp.fc2.bias', 'layers.2.blocks.9.norm1.weight', 'layers.2.blocks.9.norm1.bias', 'layers.2.blocks.9.attn.relative_position_bias_table', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.9.attn.qkv.weight', 'layers.2.blocks.9.attn.qkv.bias', 'layers.2.blocks.9.attn.proj.weight', 'layers.2.blocks.9.attn.proj.bias', 'layers.2.blocks.9.norm2.weight', 'layers.2.blocks.9.norm2.bias', 'layers.2.blocks.9.mlp.fc1.weight', 'layers.2.blocks.9.mlp.fc1.bias', 'layers.2.blocks.9.mlp.fc2.weight', 'layers.2.blocks.9.mlp.fc2.bias', 'layers.2.blocks.10.norm1.weight', 'layers.2.blocks.10.norm1.bias', 'layers.2.blocks.10.attn.relative_position_bias_table', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.10.attn.qkv.weight', 'layers.2.blocks.10.attn.qkv.bias', 'layers.2.blocks.10.attn.proj.weight', 'layers.2.blocks.10.attn.proj.bias', 'layers.2.blocks.10.norm2.weight', 'layers.2.blocks.10.norm2.bias', 'layers.2.blocks.10.mlp.fc1.weight', 'layers.2.blocks.10.mlp.fc1.bias', 'layers.2.blocks.10.mlp.fc2.weight', 'layers.2.blocks.10.mlp.fc2.bias', 'layers.2.blocks.11.norm1.weight', 'layers.2.blocks.11.norm1.bias', 'layers.2.blocks.11.attn.relative_position_bias_table', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.11.attn.qkv.weight', 'layers.2.blocks.11.attn.qkv.bias', 'layers.2.blocks.11.attn.proj.weight', 'layers.2.blocks.11.attn.proj.bias', 'layers.2.blocks.11.norm2.weight', 'layers.2.blocks.11.norm2.bias', 'layers.2.blocks.11.mlp.fc1.weight', 'layers.2.blocks.11.mlp.fc1.bias', 'layers.2.blocks.11.mlp.fc2.weight', 'layers.2.blocks.11.mlp.fc2.bias', 'layers.2.blocks.12.norm1.weight', 'layers.2.blocks.12.norm1.bias', 'layers.2.blocks.12.attn.relative_position_bias_table', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.12.attn.qkv.weight', 'layers.2.blocks.12.attn.qkv.bias', 'layers.2.blocks.12.attn.proj.weight', 'layers.2.blocks.12.attn.proj.bias', 'layers.2.blocks.12.norm2.weight', 'layers.2.blocks.12.norm2.bias', 'layers.2.blocks.12.mlp.fc1.weight', 'layers.2.blocks.12.mlp.fc1.bias', 'layers.2.blocks.12.mlp.fc2.weight', 'layers.2.blocks.12.mlp.fc2.bias', 'layers.2.blocks.13.norm1.weight', 'layers.2.blocks.13.norm1.bias', 'layers.2.blocks.13.attn.relative_position_bias_table', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.13.attn.qkv.weight', 'layers.2.blocks.13.attn.qkv.bias', 'layers.2.blocks.13.attn.proj.weight', 'layers.2.blocks.13.attn.proj.bias', 'layers.2.blocks.13.norm2.weight', 'layers.2.blocks.13.norm2.bias', 'layers.2.blocks.13.mlp.fc1.weight', 'layers.2.blocks.13.mlp.fc1.bias', 'layers.2.blocks.13.mlp.fc2.weight', 'layers.2.blocks.13.mlp.fc2.bias', 'layers.2.blocks.14.norm1.weight', 'layers.2.blocks.14.norm1.bias', 'layers.2.blocks.14.attn.relative_position_bias_table', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.14.attn.qkv.weight', 'layers.2.blocks.14.attn.qkv.bias', 'layers.2.blocks.14.attn.proj.weight', 'layers.2.blocks.14.attn.proj.bias', 'layers.2.blocks.14.norm2.weight', 'layers.2.blocks.14.norm2.bias', 'layers.2.blocks.14.mlp.fc1.weight', 'layers.2.blocks.14.mlp.fc1.bias', 'layers.2.blocks.14.mlp.fc2.weight', 'layers.2.blocks.14.mlp.fc2.bias', 'layers.2.blocks.15.norm1.weight', 'layers.2.blocks.15.norm1.bias', 'layers.2.blocks.15.attn.relative_position_bias_table', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.15.attn.qkv.weight', 'layers.2.blocks.15.attn.qkv.bias', 'layers.2.blocks.15.attn.proj.weight', 'layers.2.blocks.15.attn.proj.bias', 'layers.2.blocks.15.norm2.weight', 'layers.2.blocks.15.norm2.bias', 'layers.2.blocks.15.mlp.fc1.weight', 'layers.2.blocks.15.mlp.fc1.bias', 'layers.2.blocks.15.mlp.fc2.weight', 'layers.2.blocks.15.mlp.fc2.bias', 'layers.2.blocks.16.norm1.weight', 'layers.2.blocks.16.norm1.bias', 'layers.2.blocks.16.attn.relative_position_bias_table', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.16.attn.qkv.weight', 'layers.2.blocks.16.attn.qkv.bias', 'layers.2.blocks.16.attn.proj.weight', 'layers.2.blocks.16.attn.proj.bias', 'layers.2.blocks.16.norm2.weight', 'layers.2.blocks.16.norm2.bias', 'layers.2.blocks.16.mlp.fc1.weight', 'layers.2.blocks.16.mlp.fc1.bias', 'layers.2.blocks.16.mlp.fc2.weight', 'layers.2.blocks.16.mlp.fc2.bias', 'layers.2.blocks.17.norm1.weight', 'layers.2.blocks.17.norm1.bias', 'layers.2.blocks.17.attn.relative_position_bias_table', 'layers.2.blocks.17.attn.relative_position_index', 'layers.2.blocks.17.attn.qkv.weight', 'layers.2.blocks.17.attn.qkv.bias', 'layers.2.blocks.17.attn.proj.weight', 'layers.2.blocks.17.attn.proj.bias', 'layers.2.blocks.17.norm2.weight', 'layers.2.blocks.17.norm2.bias', 'layers.2.blocks.17.mlp.fc1.weight', 'layers.2.blocks.17.mlp.fc1.bias', 'layers.2.blocks.17.mlp.fc2.weight', 'layers.2.blocks.17.mlp.fc2.bias', 'layers.2.downsample.reduction.weight', 'layers.2.downsample.norm.weight', 'layers.2.downsample.norm.bias', 'layers.3.blocks.0.norm1.weight', 'layers.3.blocks.0.norm1.bias', 'layers.3.blocks.0.attn.relative_position_bias_table', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.attn.qkv.weight', 'layers.3.blocks.0.attn.qkv.bias', 'layers.3.blocks.0.attn.proj.weight', 'layers.3.blocks.0.attn.proj.bias', 'layers.3.blocks.0.norm2.weight', 'layers.3.blocks.0.norm2.bias', 'layers.3.blocks.0.mlp.fc1.weight', 'layers.3.blocks.0.mlp.fc1.bias', 'layers.3.blocks.0.mlp.fc2.weight', 'layers.3.blocks.0.mlp.fc2.bias', 'layers.3.blocks.1.norm1.weight', 'layers.3.blocks.1.norm1.bias', 'layers.3.blocks.1.attn.relative_position_bias_table', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.attn.qkv.weight', 'layers.3.blocks.1.attn.qkv.bias', 'layers.3.blocks.1.attn.proj.weight', 'layers.3.blocks.1.attn.proj.bias', 'layers.3.blocks.1.norm2.weight', 'layers.3.blocks.1.norm2.bias', 'layers.3.blocks.1.mlp.fc1.weight', 'layers.3.blocks.1.mlp.fc1.bias', 'layers.3.blocks.1.mlp.fc2.weight', 'layers.3.blocks.1.mlp.fc2.bias'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#good\n",
    "\n",
    "# model.eval()\n",
    "# #只保留权重\n",
    "# torch.save(model.state_dict(), 'model_no_arc.pth')\n",
    "\n",
    "\n",
    "checkpoint = torch.load('F:/gta5/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb_20220930-f8d74db7.pth')\n",
    "state_dict = collections.OrderedDict(checkpoint['state_dict'])\n",
    "checkpoint2=torch.load('F:/A-ROAD-CHALL/weights/weights_task1/pretrained_weights_task1.pth')\n",
    "# # # # model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# # # # load_checkpoint(model, checkpoint, strict=False,revise_keys=[r'^backbone\\.'])\n",
    "backbone_stat_dict = {}\n",
    "for k in list(state_dict.keys()):\n",
    "    # 去掉backbone.\n",
    "    # new_k=k\n",
    "    if k.startswith('backbone'):\n",
    "         backbone_stat_dict[k.replace('backbone.', '')] = state_dict[k]\n",
    "    # 修改层名\n",
    "    if k.startswith('backbone.layers.0'):    \n",
    "        backbone_stat_dict[k.replace('backbone.layers.0', 'swinaclayer1')] = state_dict[k]\n",
    "\n",
    "    if k.startswith('backbone.layers.1'):\n",
    "        backbone_stat_dict[k.replace('backbone.layers.1', 'swinaclayer2')] = state_dict[k]\n",
    "    \n",
    "    if k.startswith('backbone.layers.2'):\n",
    "        backbone_stat_dict[k.replace('backbone.layers.2', 'swinaclayer3')] = state_dict[k]\n",
    "    \n",
    "    if k.startswith('backbone.layers.3'):\n",
    "        backbone_stat_dict[k.replace('backbone.layers.3', 'swinaclayer4')] = state_dict[k]\n",
    "    # new_k = new_k.replace('layers.1', 'swinaclayer2')\n",
    "    # new_k = new_k.replace('layers.2', 'swinaclayer3')\n",
    "    # # print('ok')\n",
    "    # new_k = new_k.replace('layers.3', 'swinaclayer4')\n",
    "    # # 改键名\n",
    "    # checkpoint[new_k] = state_dict.pop(k)\n",
    "# model_stat_dict.update(backbone_stat_dict)\n",
    "# model.load_state_dict(model_stat_dict,strict=False)\n",
    "# backbone_stat_dict.update(state_dict)\n",
    "\n",
    "backbone_stat_dict2 = {}\n",
    "for k in list(checkpoint2.keys()):\n",
    "    if 'module.backbone.layer' or 'conv6' or 'conv7' or 'lateral' or 'corr_layer' or 'heads' in k:\n",
    "        if k.startswith('module.backbone'):\n",
    "            backbone_stat_dict2[k.replace('module.backbone.', '')] = checkpoint2[k]\n",
    "\n",
    "        # if k.startswith('module'):\n",
    "        #     backbone_stat_dict2[k.replace('module.', '')] = checkpoint2[k]\n",
    "\n",
    "\n",
    "\n",
    "new_stat_dict={}\n",
    "new_stat_dict.update(backbone_stat_dict2)\n",
    "new_stat_dict.update(backbone_stat_dict)\n",
    "# for k in list(checkpoint2.keys()):\n",
    "#     new_k=k\n",
    "#     if k.startswith('module.backbone'):\n",
    "#         new_k = k[16:]\n",
    "#         # print('ok')\n",
    "#     #改键名\n",
    "#     checkpoint2[new_k] = checkpoint2.pop(k)\n",
    "model.load_state_dict(new_stat_dict,strict=False)\n",
    "\n",
    "# new_checkpoint = {}\n",
    "# new_checkpoint.update(checkpoint) \n",
    "# new_checkpoint.update(checkpoint2)\n",
    "\n",
    "# 加载到模型中    \n",
    "# model.load_state_dict(new_checkpoint, strict=False)\n",
    "# model.load_state_dict(checkpoint2, strict=False)\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "# summary(model, input_size=(1, 3, 8, 224, 224))\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['patch_embed.proj.weight', 'patch_embed.proj.bias', 'patch_embed.norm.weight', 'patch_embed.norm.bias', 'swinaclayer1.blocks.0.norm1.weight', 'swinaclayer1.blocks.0.norm1.bias', 'swinaclayer1.blocks.0.attn.relative_position_bias_table', 'swinaclayer1.blocks.0.attn.relative_position_index', 'swinaclayer1.blocks.0.attn.qkv.weight', 'swinaclayer1.blocks.0.attn.qkv.bias', 'swinaclayer1.blocks.0.attn.proj.weight', 'swinaclayer1.blocks.0.attn.proj.bias', 'swinaclayer1.blocks.0.norm2.weight', 'swinaclayer1.blocks.0.norm2.bias', 'swinaclayer1.blocks.0.mlp.fc1.weight', 'swinaclayer1.blocks.0.mlp.fc1.bias', 'swinaclayer1.blocks.0.mlp.fc2.weight', 'swinaclayer1.blocks.0.mlp.fc2.bias', 'swinaclayer1.blocks.1.norm1.weight', 'swinaclayer1.blocks.1.norm1.bias', 'swinaclayer1.blocks.1.attn.relative_position_bias_table', 'swinaclayer1.blocks.1.attn.relative_position_index', 'swinaclayer1.blocks.1.attn.qkv.weight', 'swinaclayer1.blocks.1.attn.qkv.bias', 'swinaclayer1.blocks.1.attn.proj.weight', 'swinaclayer1.blocks.1.attn.proj.bias', 'swinaclayer1.blocks.1.norm2.weight', 'swinaclayer1.blocks.1.norm2.bias', 'swinaclayer1.blocks.1.mlp.fc1.weight', 'swinaclayer1.blocks.1.mlp.fc1.bias', 'swinaclayer1.blocks.1.mlp.fc2.weight', 'swinaclayer1.blocks.1.mlp.fc2.bias', 'swinaclayer1.downsample.reduction.weight', 'swinaclayer1.downsample.norm.weight', 'swinaclayer1.downsample.norm.bias', 'swinaclayer2.blocks.0.norm1.weight', 'swinaclayer2.blocks.0.norm1.bias', 'swinaclayer2.blocks.0.attn.relative_position_bias_table', 'swinaclayer2.blocks.0.attn.relative_position_index', 'swinaclayer2.blocks.0.attn.qkv.weight', 'swinaclayer2.blocks.0.attn.qkv.bias', 'swinaclayer2.blocks.0.attn.proj.weight', 'swinaclayer2.blocks.0.attn.proj.bias', 'swinaclayer2.blocks.0.norm2.weight', 'swinaclayer2.blocks.0.norm2.bias', 'swinaclayer2.blocks.0.mlp.fc1.weight', 'swinaclayer2.blocks.0.mlp.fc1.bias', 'swinaclayer2.blocks.0.mlp.fc2.weight', 'swinaclayer2.blocks.0.mlp.fc2.bias', 'swinaclayer2.blocks.1.norm1.weight', 'swinaclayer2.blocks.1.norm1.bias', 'swinaclayer2.blocks.1.attn.relative_position_bias_table', 'swinaclayer2.blocks.1.attn.relative_position_index', 'swinaclayer2.blocks.1.attn.qkv.weight', 'swinaclayer2.blocks.1.attn.qkv.bias', 'swinaclayer2.blocks.1.attn.proj.weight', 'swinaclayer2.blocks.1.attn.proj.bias', 'swinaclayer2.blocks.1.norm2.weight', 'swinaclayer2.blocks.1.norm2.bias', 'swinaclayer2.blocks.1.mlp.fc1.weight', 'swinaclayer2.blocks.1.mlp.fc1.bias', 'swinaclayer2.blocks.1.mlp.fc2.weight', 'swinaclayer2.blocks.1.mlp.fc2.bias', 'swinaclayer2.downsample.reduction.weight', 'swinaclayer2.downsample.norm.weight', 'swinaclayer2.downsample.norm.bias', 'swinaclayer3.blocks.0.norm1.weight', 'swinaclayer3.blocks.0.norm1.bias', 'swinaclayer3.blocks.0.attn.relative_position_bias_table', 'swinaclayer3.blocks.0.attn.relative_position_index', 'swinaclayer3.blocks.0.attn.qkv.weight', 'swinaclayer3.blocks.0.attn.qkv.bias', 'swinaclayer3.blocks.0.attn.proj.weight', 'swinaclayer3.blocks.0.attn.proj.bias', 'swinaclayer3.blocks.0.norm2.weight', 'swinaclayer3.blocks.0.norm2.bias', 'swinaclayer3.blocks.0.mlp.fc1.weight', 'swinaclayer3.blocks.0.mlp.fc1.bias', 'swinaclayer3.blocks.0.mlp.fc2.weight', 'swinaclayer3.blocks.0.mlp.fc2.bias', 'swinaclayer3.blocks.1.norm1.weight', 'swinaclayer3.blocks.1.norm1.bias', 'swinaclayer3.blocks.1.attn.relative_position_bias_table', 'swinaclayer3.blocks.1.attn.relative_position_index', 'swinaclayer3.blocks.1.attn.qkv.weight', 'swinaclayer3.blocks.1.attn.qkv.bias', 'swinaclayer3.blocks.1.attn.proj.weight', 'swinaclayer3.blocks.1.attn.proj.bias', 'swinaclayer3.blocks.1.norm2.weight', 'swinaclayer3.blocks.1.norm2.bias', 'swinaclayer3.blocks.1.mlp.fc1.weight', 'swinaclayer3.blocks.1.mlp.fc1.bias', 'swinaclayer3.blocks.1.mlp.fc2.weight', 'swinaclayer3.blocks.1.mlp.fc2.bias', 'swinaclayer3.blocks.2.norm1.weight', 'swinaclayer3.blocks.2.norm1.bias', 'swinaclayer3.blocks.2.attn.relative_position_bias_table', 'swinaclayer3.blocks.2.attn.relative_position_index', 'swinaclayer3.blocks.2.attn.qkv.weight', 'swinaclayer3.blocks.2.attn.qkv.bias', 'swinaclayer3.blocks.2.attn.proj.weight', 'swinaclayer3.blocks.2.attn.proj.bias', 'swinaclayer3.blocks.2.norm2.weight', 'swinaclayer3.blocks.2.norm2.bias', 'swinaclayer3.blocks.2.mlp.fc1.weight', 'swinaclayer3.blocks.2.mlp.fc1.bias', 'swinaclayer3.blocks.2.mlp.fc2.weight', 'swinaclayer3.blocks.2.mlp.fc2.bias', 'swinaclayer3.blocks.3.norm1.weight', 'swinaclayer3.blocks.3.norm1.bias', 'swinaclayer3.blocks.3.attn.relative_position_bias_table', 'swinaclayer3.blocks.3.attn.relative_position_index', 'swinaclayer3.blocks.3.attn.qkv.weight', 'swinaclayer3.blocks.3.attn.qkv.bias', 'swinaclayer3.blocks.3.attn.proj.weight', 'swinaclayer3.blocks.3.attn.proj.bias', 'swinaclayer3.blocks.3.norm2.weight', 'swinaclayer3.blocks.3.norm2.bias', 'swinaclayer3.blocks.3.mlp.fc1.weight', 'swinaclayer3.blocks.3.mlp.fc1.bias', 'swinaclayer3.blocks.3.mlp.fc2.weight', 'swinaclayer3.blocks.3.mlp.fc2.bias', 'swinaclayer3.blocks.4.norm1.weight', 'swinaclayer3.blocks.4.norm1.bias', 'swinaclayer3.blocks.4.attn.relative_position_bias_table', 'swinaclayer3.blocks.4.attn.relative_position_index', 'swinaclayer3.blocks.4.attn.qkv.weight', 'swinaclayer3.blocks.4.attn.qkv.bias', 'swinaclayer3.blocks.4.attn.proj.weight', 'swinaclayer3.blocks.4.attn.proj.bias', 'swinaclayer3.blocks.4.norm2.weight', 'swinaclayer3.blocks.4.norm2.bias', 'swinaclayer3.blocks.4.mlp.fc1.weight', 'swinaclayer3.blocks.4.mlp.fc1.bias', 'swinaclayer3.blocks.4.mlp.fc2.weight', 'swinaclayer3.blocks.4.mlp.fc2.bias', 'swinaclayer3.blocks.5.norm1.weight', 'swinaclayer3.blocks.5.norm1.bias', 'swinaclayer3.blocks.5.attn.relative_position_bias_table', 'swinaclayer3.blocks.5.attn.relative_position_index', 'swinaclayer3.blocks.5.attn.qkv.weight', 'swinaclayer3.blocks.5.attn.qkv.bias', 'swinaclayer3.blocks.5.attn.proj.weight', 'swinaclayer3.blocks.5.attn.proj.bias', 'swinaclayer3.blocks.5.norm2.weight', 'swinaclayer3.blocks.5.norm2.bias', 'swinaclayer3.blocks.5.mlp.fc1.weight', 'swinaclayer3.blocks.5.mlp.fc1.bias', 'swinaclayer3.blocks.5.mlp.fc2.weight', 'swinaclayer3.blocks.5.mlp.fc2.bias', 'swinaclayer3.blocks.6.norm1.weight', 'swinaclayer3.blocks.6.norm1.bias', 'swinaclayer3.blocks.6.attn.relative_position_bias_table', 'swinaclayer3.blocks.6.attn.relative_position_index', 'swinaclayer3.blocks.6.attn.qkv.weight', 'swinaclayer3.blocks.6.attn.qkv.bias', 'swinaclayer3.blocks.6.attn.proj.weight', 'swinaclayer3.blocks.6.attn.proj.bias', 'swinaclayer3.blocks.6.norm2.weight', 'swinaclayer3.blocks.6.norm2.bias', 'swinaclayer3.blocks.6.mlp.fc1.weight', 'swinaclayer3.blocks.6.mlp.fc1.bias', 'swinaclayer3.blocks.6.mlp.fc2.weight', 'swinaclayer3.blocks.6.mlp.fc2.bias', 'swinaclayer3.blocks.7.norm1.weight', 'swinaclayer3.blocks.7.norm1.bias', 'swinaclayer3.blocks.7.attn.relative_position_bias_table', 'swinaclayer3.blocks.7.attn.relative_position_index', 'swinaclayer3.blocks.7.attn.qkv.weight', 'swinaclayer3.blocks.7.attn.qkv.bias', 'swinaclayer3.blocks.7.attn.proj.weight', 'swinaclayer3.blocks.7.attn.proj.bias', 'swinaclayer3.blocks.7.norm2.weight', 'swinaclayer3.blocks.7.norm2.bias', 'swinaclayer3.blocks.7.mlp.fc1.weight', 'swinaclayer3.blocks.7.mlp.fc1.bias', 'swinaclayer3.blocks.7.mlp.fc2.weight', 'swinaclayer3.blocks.7.mlp.fc2.bias', 'swinaclayer3.blocks.8.norm1.weight', 'swinaclayer3.blocks.8.norm1.bias', 'swinaclayer3.blocks.8.attn.relative_position_bias_table', 'swinaclayer3.blocks.8.attn.relative_position_index', 'swinaclayer3.blocks.8.attn.qkv.weight', 'swinaclayer3.blocks.8.attn.qkv.bias', 'swinaclayer3.blocks.8.attn.proj.weight', 'swinaclayer3.blocks.8.attn.proj.bias', 'swinaclayer3.blocks.8.norm2.weight', 'swinaclayer3.blocks.8.norm2.bias', 'swinaclayer3.blocks.8.mlp.fc1.weight', 'swinaclayer3.blocks.8.mlp.fc1.bias', 'swinaclayer3.blocks.8.mlp.fc2.weight', 'swinaclayer3.blocks.8.mlp.fc2.bias', 'swinaclayer3.blocks.9.norm1.weight', 'swinaclayer3.blocks.9.norm1.bias', 'swinaclayer3.blocks.9.attn.relative_position_bias_table', 'swinaclayer3.blocks.9.attn.relative_position_index', 'swinaclayer3.blocks.9.attn.qkv.weight', 'swinaclayer3.blocks.9.attn.qkv.bias', 'swinaclayer3.blocks.9.attn.proj.weight', 'swinaclayer3.blocks.9.attn.proj.bias', 'swinaclayer3.blocks.9.norm2.weight', 'swinaclayer3.blocks.9.norm2.bias', 'swinaclayer3.blocks.9.mlp.fc1.weight', 'swinaclayer3.blocks.9.mlp.fc1.bias', 'swinaclayer3.blocks.9.mlp.fc2.weight', 'swinaclayer3.blocks.9.mlp.fc2.bias', 'swinaclayer3.blocks.10.norm1.weight', 'swinaclayer3.blocks.10.norm1.bias', 'swinaclayer3.blocks.10.attn.relative_position_bias_table', 'swinaclayer3.blocks.10.attn.relative_position_index', 'swinaclayer3.blocks.10.attn.qkv.weight', 'swinaclayer3.blocks.10.attn.qkv.bias', 'swinaclayer3.blocks.10.attn.proj.weight', 'swinaclayer3.blocks.10.attn.proj.bias', 'swinaclayer3.blocks.10.norm2.weight', 'swinaclayer3.blocks.10.norm2.bias', 'swinaclayer3.blocks.10.mlp.fc1.weight', 'swinaclayer3.blocks.10.mlp.fc1.bias', 'swinaclayer3.blocks.10.mlp.fc2.weight', 'swinaclayer3.blocks.10.mlp.fc2.bias', 'swinaclayer3.blocks.11.norm1.weight', 'swinaclayer3.blocks.11.norm1.bias', 'swinaclayer3.blocks.11.attn.relative_position_bias_table', 'swinaclayer3.blocks.11.attn.relative_position_index', 'swinaclayer3.blocks.11.attn.qkv.weight', 'swinaclayer3.blocks.11.attn.qkv.bias', 'swinaclayer3.blocks.11.attn.proj.weight', 'swinaclayer3.blocks.11.attn.proj.bias', 'swinaclayer3.blocks.11.norm2.weight', 'swinaclayer3.blocks.11.norm2.bias', 'swinaclayer3.blocks.11.mlp.fc1.weight', 'swinaclayer3.blocks.11.mlp.fc1.bias', 'swinaclayer3.blocks.11.mlp.fc2.weight', 'swinaclayer3.blocks.11.mlp.fc2.bias', 'swinaclayer3.blocks.12.norm1.weight', 'swinaclayer3.blocks.12.norm1.bias', 'swinaclayer3.blocks.12.attn.relative_position_bias_table', 'swinaclayer3.blocks.12.attn.relative_position_index', 'swinaclayer3.blocks.12.attn.qkv.weight', 'swinaclayer3.blocks.12.attn.qkv.bias', 'swinaclayer3.blocks.12.attn.proj.weight', 'swinaclayer3.blocks.12.attn.proj.bias', 'swinaclayer3.blocks.12.norm2.weight', 'swinaclayer3.blocks.12.norm2.bias', 'swinaclayer3.blocks.12.mlp.fc1.weight', 'swinaclayer3.blocks.12.mlp.fc1.bias', 'swinaclayer3.blocks.12.mlp.fc2.weight', 'swinaclayer3.blocks.12.mlp.fc2.bias', 'swinaclayer3.blocks.13.norm1.weight', 'swinaclayer3.blocks.13.norm1.bias', 'swinaclayer3.blocks.13.attn.relative_position_bias_table', 'swinaclayer3.blocks.13.attn.relative_position_index', 'swinaclayer3.blocks.13.attn.qkv.weight', 'swinaclayer3.blocks.13.attn.qkv.bias', 'swinaclayer3.blocks.13.attn.proj.weight', 'swinaclayer3.blocks.13.attn.proj.bias', 'swinaclayer3.blocks.13.norm2.weight', 'swinaclayer3.blocks.13.norm2.bias', 'swinaclayer3.blocks.13.mlp.fc1.weight', 'swinaclayer3.blocks.13.mlp.fc1.bias', 'swinaclayer3.blocks.13.mlp.fc2.weight', 'swinaclayer3.blocks.13.mlp.fc2.bias', 'swinaclayer3.blocks.14.norm1.weight', 'swinaclayer3.blocks.14.norm1.bias', 'swinaclayer3.blocks.14.attn.relative_position_bias_table', 'swinaclayer3.blocks.14.attn.relative_position_index', 'swinaclayer3.blocks.14.attn.qkv.weight', 'swinaclayer3.blocks.14.attn.qkv.bias', 'swinaclayer3.blocks.14.attn.proj.weight', 'swinaclayer3.blocks.14.attn.proj.bias', 'swinaclayer3.blocks.14.norm2.weight', 'swinaclayer3.blocks.14.norm2.bias', 'swinaclayer3.blocks.14.mlp.fc1.weight', 'swinaclayer3.blocks.14.mlp.fc1.bias', 'swinaclayer3.blocks.14.mlp.fc2.weight', 'swinaclayer3.blocks.14.mlp.fc2.bias', 'swinaclayer3.blocks.15.norm1.weight', 'swinaclayer3.blocks.15.norm1.bias', 'swinaclayer3.blocks.15.attn.relative_position_bias_table', 'swinaclayer3.blocks.15.attn.relative_position_index', 'swinaclayer3.blocks.15.attn.qkv.weight', 'swinaclayer3.blocks.15.attn.qkv.bias', 'swinaclayer3.blocks.15.attn.proj.weight', 'swinaclayer3.blocks.15.attn.proj.bias', 'swinaclayer3.blocks.15.norm2.weight', 'swinaclayer3.blocks.15.norm2.bias', 'swinaclayer3.blocks.15.mlp.fc1.weight', 'swinaclayer3.blocks.15.mlp.fc1.bias', 'swinaclayer3.blocks.15.mlp.fc2.weight', 'swinaclayer3.blocks.15.mlp.fc2.bias', 'swinaclayer3.blocks.16.norm1.weight', 'swinaclayer3.blocks.16.norm1.bias', 'swinaclayer3.blocks.16.attn.relative_position_bias_table', 'swinaclayer3.blocks.16.attn.relative_position_index', 'swinaclayer3.blocks.16.attn.qkv.weight', 'swinaclayer3.blocks.16.attn.qkv.bias', 'swinaclayer3.blocks.16.attn.proj.weight', 'swinaclayer3.blocks.16.attn.proj.bias', 'swinaclayer3.blocks.16.norm2.weight', 'swinaclayer3.blocks.16.norm2.bias', 'swinaclayer3.blocks.16.mlp.fc1.weight', 'swinaclayer3.blocks.16.mlp.fc1.bias', 'swinaclayer3.blocks.16.mlp.fc2.weight', 'swinaclayer3.blocks.16.mlp.fc2.bias', 'swinaclayer3.blocks.17.norm1.weight', 'swinaclayer3.blocks.17.norm1.bias', 'swinaclayer3.blocks.17.attn.relative_position_bias_table', 'swinaclayer3.blocks.17.attn.relative_position_index', 'swinaclayer3.blocks.17.attn.qkv.weight', 'swinaclayer3.blocks.17.attn.qkv.bias', 'swinaclayer3.blocks.17.attn.proj.weight', 'swinaclayer3.blocks.17.attn.proj.bias', 'swinaclayer3.blocks.17.norm2.weight', 'swinaclayer3.blocks.17.norm2.bias', 'swinaclayer3.blocks.17.mlp.fc1.weight', 'swinaclayer3.blocks.17.mlp.fc1.bias', 'swinaclayer3.blocks.17.mlp.fc2.weight', 'swinaclayer3.blocks.17.mlp.fc2.bias', 'swinaclayer3.downsample.reduction.weight', 'swinaclayer3.downsample.norm.weight', 'swinaclayer3.downsample.norm.bias', 'swinaclayer4.blocks.0.norm1.weight', 'swinaclayer4.blocks.0.norm1.bias', 'swinaclayer4.blocks.0.attn.relative_position_bias_table', 'swinaclayer4.blocks.0.attn.relative_position_index', 'swinaclayer4.blocks.0.attn.qkv.weight', 'swinaclayer4.blocks.0.attn.qkv.bias', 'swinaclayer4.blocks.0.attn.proj.weight', 'swinaclayer4.blocks.0.attn.proj.bias', 'swinaclayer4.blocks.0.norm2.weight', 'swinaclayer4.blocks.0.norm2.bias', 'swinaclayer4.blocks.0.mlp.fc1.weight', 'swinaclayer4.blocks.0.mlp.fc1.bias', 'swinaclayer4.blocks.0.mlp.fc2.weight', 'swinaclayer4.blocks.0.mlp.fc2.bias', 'swinaclayer4.blocks.1.norm1.weight', 'swinaclayer4.blocks.1.norm1.bias', 'swinaclayer4.blocks.1.attn.relative_position_bias_table', 'swinaclayer4.blocks.1.attn.relative_position_index', 'swinaclayer4.blocks.1.attn.qkv.weight', 'swinaclayer4.blocks.1.attn.qkv.bias', 'swinaclayer4.blocks.1.attn.proj.weight', 'swinaclayer4.blocks.1.attn.proj.bias', 'swinaclayer4.blocks.1.norm2.weight', 'swinaclayer4.blocks.1.norm2.bias', 'swinaclayer4.blocks.1.mlp.fc1.weight', 'swinaclayer4.blocks.1.mlp.fc1.bias', 'swinaclayer4.blocks.1.mlp.fc2.weight', 'swinaclayer4.blocks.1.mlp.fc2.bias', 'norm3.weight', 'norm3.bias', 'upcov1.weight', 'upcov2.weight', 'upcov2.bias', 'upcov3.weight', 'upcov3.bias', 'upcov4.weight', 'layer1.0.conv1.weight', 'layer1.0.cgru.recurrent_conv_gates.weight', 'layer1.0.cgru.recurrent_conv_gates.bias', 'layer1.0.cgru.recurrent_conv_out.weight', 'layer1.0.cgru.recurrent_conv_out.bias', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.bn3.bias', 'layer1.0.bn3.running_mean', 'layer1.0.bn3.running_var', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.1.conv1.weight', 'layer1.1.cgru.recurrent_conv_gates.weight', 'layer1.1.cgru.recurrent_conv_gates.bias', 'layer1.1.cgru.recurrent_conv_out.weight', 'layer1.1.cgru.recurrent_conv_out.bias', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.1.bn3.bias', 'layer1.1.bn3.running_mean', 'layer1.1.bn3.running_var', 'layer1.2.conv1.weight', 'layer1.2.cgru.recurrent_conv_gates.weight', 'layer1.2.cgru.recurrent_conv_gates.bias', 'layer1.2.cgru.recurrent_conv_out.weight', 'layer1.2.cgru.recurrent_conv_out.bias', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer1.2.bn3.bias', 'layer1.2.bn3.running_mean', 'layer1.2.bn3.running_var', 'layer2.0.conv1.weight', 'layer2.0.cgru.recurrent_conv_gates.weight', 'layer2.0.cgru.recurrent_conv_gates.bias', 'layer2.0.cgru.recurrent_conv_out.weight', 'layer2.0.cgru.recurrent_conv_out.bias', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn3.bias', 'layer2.0.bn3.running_mean', 'layer2.0.bn3.running_var', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.1.bn3.bias', 'layer2.1.bn3.running_mean', 'layer2.1.bn3.running_var', 'layer2.2.conv1.weight', 'layer2.2.cgru.recurrent_conv_gates.weight', 'layer2.2.cgru.recurrent_conv_gates.bias', 'layer2.2.cgru.recurrent_conv_out.weight', 'layer2.2.cgru.recurrent_conv_out.bias', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.2.bn3.bias', 'layer2.2.bn3.running_mean', 'layer2.2.bn3.running_var', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn3.bias', 'layer2.3.bn3.running_mean', 'layer2.3.bn3.running_var', 'layer3.0.conv1.weight', 'layer3.0.cgru.recurrent_conv_gates.weight', 'layer3.0.cgru.recurrent_conv_gates.bias', 'layer3.0.cgru.recurrent_conv_out.weight', 'layer3.0.cgru.recurrent_conv_out.bias', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.bn3.bias', 'layer3.0.bn3.running_mean', 'layer3.0.bn3.running_var', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.1.bn3.bias', 'layer3.1.bn3.running_mean', 'layer3.1.bn3.running_var', 'layer3.2.conv1.weight', 'layer3.2.cgru.recurrent_conv_gates.weight', 'layer3.2.cgru.recurrent_conv_gates.bias', 'layer3.2.cgru.recurrent_conv_out.weight', 'layer3.2.cgru.recurrent_conv_out.bias', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.2.bn3.bias', 'layer3.2.bn3.running_mean', 'layer3.2.bn3.running_var', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.3.bn3.bias', 'layer3.3.bn3.running_mean', 'layer3.3.bn3.running_var', 'layer3.4.conv1.weight', 'layer3.4.cgru.recurrent_conv_gates.weight', 'layer3.4.cgru.recurrent_conv_gates.bias', 'layer3.4.cgru.recurrent_conv_out.weight', 'layer3.4.cgru.recurrent_conv_out.bias', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.4.bn3.bias', 'layer3.4.bn3.running_mean', 'layer3.4.bn3.running_var', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer3.5.bn3.bias', 'layer3.5.bn3.running_mean', 'layer3.5.bn3.running_var', 'layer4.0.conv1.weight', 'layer4.0.cgru.recurrent_conv_gates.weight', 'layer4.0.cgru.recurrent_conv_gates.bias', 'layer4.0.cgru.recurrent_conv_out.weight', 'layer4.0.cgru.recurrent_conv_out.bias', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.bn3.bias', 'layer4.0.bn3.running_mean', 'layer4.0.bn3.running_var', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.1.conv1.weight', 'layer4.1.cgru.recurrent_conv_gates.weight', 'layer4.1.cgru.recurrent_conv_gates.bias', 'layer4.1.cgru.recurrent_conv_out.weight', 'layer4.1.cgru.recurrent_conv_out.bias', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn3.bias', 'layer4.1.bn3.running_mean', 'layer4.1.bn3.running_var', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias', 'layer4.2.bn3.running_mean', 'layer4.2.bn3.running_var', 'conv6.weight', 'conv7.weight', 'lateral_layer1.weight', 'lateral_layer2.weight', 'lateral_layer3.weight', 'corr_layer1.weight', 'corr_layer2.weight', 'corr_layer3.weight'], unexpected_keys=['meta', 'state_dict'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "# #只保留权重\n",
    "# torch.save(model.state_dict(), 'model_no_arc.pth')\n",
    "\n",
    "\n",
    "checkpoint = torch.load('F:/gta5/swin-large-p244-w877_in22k-pre_16xb8-amp-32x2x1-30e_kinetics700-rgb_20220930-f8d74db7.pth')\n",
    "\n",
    "checkpoint2=torch.load('F:/A-ROAD-CHALL/weights/weights_task1/pretrained_weights_task1.pth')\n",
    "# # # # model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# # # # load_checkpoint(model, checkpoint, strict=False,revise_keys=[r'^backbone\\.'])\n",
    "\n",
    "for k in list(checkpoint.keys()):\n",
    "    # 去掉backbone.\n",
    "    new_k=k\n",
    "    if k.startswith('backbone'):\n",
    "        new_k = k[9:] \n",
    "    # 修改层名    \n",
    "    new_k = new_k.replace('layers.0', 'swinaclayer1')\n",
    "    new_k = new_k.replace('layers.1', 'swinaclayer2')\n",
    "    new_k = new_k.replace('layers.2', 'swinaclayer3')\n",
    "    # print('ok')\n",
    "    new_k = new_k.replace('layers.3', 'swinaclayer4')\n",
    "    # 改键名\n",
    "    checkpoint[new_k] = checkpoint.pop(k)\n",
    "\n",
    "\n",
    "\n",
    "for k in list(checkpoint2.keys()):\n",
    "    new_k=k\n",
    "    if k.startswith('module.backbone'):\n",
    "        new_k = k[16:]\n",
    "        # print('ok')\n",
    "    #改键名\n",
    "    checkpoint2[new_k] = checkpoint2.pop(k)\n",
    "\n",
    "\n",
    "# new_checkpoint = {}\n",
    "# new_checkpoint.update(checkpoint) \n",
    "# new_checkpoint.update(checkpoint2)\n",
    "\n",
    "# 加载到模型中    \n",
    "# model.load_state_dict(new_checkpoint, strict=False)\n",
    "# model.load_state_dict(checkpoint2, strict=False)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "\n",
    "# summary(model, input_size=(1, 3, 8, 224, 224))\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "FPN network Classes\n",
    "\n",
    "Author: Gurkirt Singh\n",
    "Inspired from https://github.com/kuangliu/pytorch-retinanet and\n",
    "https://github.com/gurkirt/realtime-action-detection\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import modules.utils as utils\n",
    "# from backbone_models import backbone_models\n",
    "from modules.anchor_box_kmeans import anchorBox as KanchorBox\n",
    "from modules.anchor_box_retinanet import anchorBox as RanchorBox\n",
    "from modules.box_utils import decode\n",
    "from modules.detection_loss import FocalLoss\n",
    "\n",
    "logger = utils.get_logger(__name__)\n",
    "\n",
    "class RetinaNet(nn.Module):\n",
    "    \"\"\"Feature Pyramid Network Architecture\n",
    "    The network is composed of a backbone FPN network followed by the\n",
    "    added Head conv layers.  \n",
    "    Each head layer branches into\n",
    "        1) conv2d for class conf scores\n",
    "        2) conv2d for localization predictions\n",
    "    See: \n",
    "    RetinaNet: https://arxiv.org/pdf/1708.02002.pdf for more details.\n",
    "    FPN: https://arxiv.org/pdf/1612.03144.pdf\n",
    "\n",
    "    Args:\n",
    "        backbone Network:\n",
    "        Program Argument Namespace\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone, args):\n",
    "        super(RetinaNet, self).__init__()\n",
    "\n",
    "        self.num_classes = args.num_classes\n",
    "        # TODO: implement __call__ i\n",
    "\n",
    "        if args.ANCHOR_TYPE == 'RETINA':\n",
    "            self.anchors = RanchorBox()\n",
    "        elif args.ANCHOR_TYPE == 'KMEANS':\n",
    "            self.anchors = KanchorBox()\n",
    "        else:\n",
    "            raise RuntimeError('Define correct anchor type')\n",
    "            \n",
    "        # print('Cell anchors\\n', self.anchors.cell_anchors)\n",
    "        # pdb.set_trace()\n",
    "        self.ar = self.anchors.ar\n",
    "        args.ar = self.ar\n",
    "        self.use_bias = True\n",
    "        self.head_size = 256\n",
    "        self.backbone = backbone\n",
    "        self.SEQ_LEN = args.SEQ_LEN\n",
    "        self.HEAD_LAYERS = args.HEAD_LAYERS\n",
    "        self.NUM_FEATURE_MAPS = args.NUM_FEATURE_MAPS\n",
    "        \n",
    "        self.reg_heads = []\n",
    "        self.cls_heads = []\n",
    "        self.prior_prob = 0.01\n",
    "        bias_value = -math.log((1 - self.prior_prob) / self.prior_prob)\n",
    "        # for nf in range(self.NUM_FEATURE_MAPS):\n",
    "        self.reg_heads = self.make_head(\n",
    "            self.ar * 4, args.REG_HEAD_TIME_SIZE, self.HEAD_LAYERS)\n",
    "        self.cls_heads = self.make_head(\n",
    "            self.ar * self.num_classes, args.CLS_HEAD_TIME_SIZE, self.HEAD_LAYERS)\n",
    "        \n",
    "        nn.init.constant_(self.cls_heads[-1].bias, bias_value)\n",
    "\n",
    "        if args.MODE == 'train':  # eval_iters only in test case\n",
    "            self.criterion = FocalLoss(args)\n",
    "\n",
    "        # self.ego_head = nn.Conv3d(self.head_size, args.num_ego_classes, kernel_size=(\n",
    "        #     3, 1, 1), stride=1, padding=(1, 0, 0))\n",
    "        # nn.init.constant_(self.ego_head.bias, bias_value)\n",
    "\n",
    "\n",
    "    # def forward(self, images, gt_boxes=None, gt_labels=None, ego_labels=None, counts=None, img_indexs=None, get_features=False):\n",
    "    def forward(self, images, gt_boxes=None, gt_labels=None, counts=None, img_indexs=None, get_features=False, logic=None, Cplus=None, Cminus=None, is_ulb=None):\n",
    "        # sources, ego_feat = self.backbone(images)\n",
    "        sources = self.backbone(images)\n",
    "\n",
    "        # ego_preds = self.ego_head(\n",
    "        #     ego_feat).squeeze(-1).squeeze(-1).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        grid_sizes = [feature_map.shape[-2:] for feature_map in sources]\n",
    "        ancohor_boxes = self.anchors(grid_sizes)\n",
    "        \n",
    "        loc = list()\n",
    "        conf = list()\n",
    "\n",
    "        for x in sources:\n",
    "            loc.append(self.reg_heads(x).permute(0, 2, 3, 4, 1).contiguous())\n",
    "            conf.append(self.cls_heads(x).permute(0, 2, 3, 4, 1).contiguous())\n",
    "\n",
    "        loc = torch.cat([o.view(o.size(0), o.size(1), -1) for o in loc], 2)\n",
    "        conf = torch.cat([o.view(o.size(0), o.size(1), -1) for o in conf], 2)\n",
    "\n",
    "        flat_loc = loc.view(loc.size(0), loc.size(1), -1, 4)\n",
    "        flat_conf = conf.view(conf.size(0), conf.size(1), -1, self.num_classes)\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        if get_features:  # testing mode with feature return\n",
    "            return flat_conf, features\n",
    "        elif gt_boxes is not None:  # training mode\n",
    "            # return self.criterion(flat_conf, flat_loc, gt_boxes, gt_labels, counts, ancohor_boxes, ego_preds, ego_labels)\n",
    "            # return self.criterion(flat_conf, flat_loc, gt_boxes, gt_labels, counts, ancohor_boxes)\n",
    "            return self.criterion(flat_conf, flat_loc, gt_boxes, gt_labels, counts, ancohor_boxes, logic, Cplus, Cminus, is_ulb=is_ulb), is_ulb\n",
    "\n",
    "        else:  # otherwise testing mode\n",
    "            decoded_boxes = []\n",
    "            for b in range(flat_loc.shape[0]):\n",
    "                temp_l = []\n",
    "                for s in range(flat_loc.shape[1]):\n",
    "                    # torch.stack([decode(flat_loc[b], ancohor_boxes) for b in range(flat_loc.shape[0])], 0),\n",
    "                    temp_l.append(decode(flat_loc[b, s], ancohor_boxes))\n",
    "                decoded_boxes.append(torch.stack(temp_l, 0))\n",
    "            # return torch.stack(decoded_boxes, 0), flat_conf, ego_preds\n",
    "            return torch.stack(decoded_boxes, 0), flat_conf\n",
    "\n",
    "\n",
    "    def make_features(self,  shared_heads):\n",
    "        layers = []\n",
    "        use_bias = self.use_bias\n",
    "        head_size = self.head_size\n",
    "        for _ in range(shared_heads):\n",
    "            layers.append(nn.Conv3d(head_size, head_size, kernel_size=(\n",
    "                1, 3, 3), stride=1, padding=(0, 1, 1), bias=use_bias))\n",
    "            layers.append(nn.ReLU(True))\n",
    "\n",
    "        layers = nn.Sequential(*layers)\n",
    "\n",
    "        for m in layers.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                if hasattr(m.bias, 'data'):\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def make_head(self, out_planes, time_kernel,  num_heads_layers):\n",
    "        layers = []\n",
    "        use_bias = self.use_bias\n",
    "        head_size = self.head_size\n",
    "        \n",
    "        for kk in range(num_heads_layers):\n",
    "            # if kk % 2 == 1 and time_kernel>1:\n",
    "            #     branch_kernel = 3\n",
    "            #     bpad = 1\n",
    "            # else:\n",
    "            branch_kernel = 1\n",
    "            bpad = 0\n",
    "            layers.append(nn.Conv3d(head_size, head_size, kernel_size=(\n",
    "                branch_kernel, 3, 3), stride=1, padding=(bpad, 1, 1), bias=use_bias))\n",
    "            layers.append(nn.ReLU(True))\n",
    "\n",
    "        tpad = time_kernel//2\n",
    "        layers.append(nn.Conv3d(head_size, out_planes, kernel_size=(\n",
    "            time_kernel, 3, 3), stride=1, padding=(tpad, 1, 1)))\n",
    "        \n",
    "        layers = nn.Sequential(*layers)\n",
    "\n",
    "        for m in layers.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                if hasattr(m.bias, 'data'):\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        return layers\n",
    "\n",
    "\n",
    "# def build_retinanet(args):\n",
    "#     return RetinaNet(backbone_models(args), args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ResNetFPN                                     [1, 256, 4, 28, 28]       28,100,928\n",
       "├─PatchEmbed3D: 1-1                           [1, 192, 4, 56, 56]       --\n",
       "│    └─Conv3d: 2-1                            [1, 192, 4, 56, 56]       18,624\n",
       "│    └─LayerNorm: 2-2                         [1, 12544, 192]           384\n",
       "├─Dropout: 1-2                                [1, 192, 4, 56, 56]       --\n",
       "├─BasicLayer: 1-3                             [1, 384, 4, 28, 28]       --\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─SwinTransformerBlock3D: 3-1       [1, 4, 56, 56, 192]       --\n",
       "│    │    │    └─LayerNorm: 4-1               [1, 4, 56, 56, 192]       384\n",
       "│    │    │    └─WindowAttention3D: 4-2       [64, 196, 192]            15,210\n",
       "│    │    │    │    └─Linear: 5-1             [64, 196, 576]            111,168\n",
       "│    │    │    │    └─Softmax: 5-2            [64, 6, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-3            [64, 6, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-4             [64, 196, 192]            37,056\n",
       "│    │    │    │    └─Dropout: 5-5            [64, 196, 192]            --\n",
       "│    │    │    └─Identity: 4-3                [1, 4, 56, 56, 192]       --\n",
       "│    │    │    └─LayerNorm: 4-4               [1, 4, 56, 56, 192]       384\n",
       "│    │    │    └─Mlp: 4-5                     [1, 4, 56, 56, 192]       --\n",
       "│    │    │    │    └─Linear: 5-6             [1, 4, 56, 56, 768]       148,224\n",
       "│    │    │    │    └─GELU: 5-7               [1, 4, 56, 56, 768]       --\n",
       "│    │    │    │    └─Dropout: 5-8            [1, 4, 56, 56, 768]       --\n",
       "│    │    │    │    └─Linear: 5-9             [1, 4, 56, 56, 192]       147,648\n",
       "│    │    │    │    └─Dropout: 5-10           [1, 4, 56, 56, 192]       --\n",
       "│    │    │    └─Identity: 4-6                [1, 4, 56, 56, 192]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-2       [1, 4, 56, 56, 192]       --\n",
       "│    │    │    └─LayerNorm: 4-7               [1, 4, 56, 56, 192]       384\n",
       "│    │    │    └─WindowAttention3D: 4-8       [64, 196, 192]            15,210\n",
       "│    │    │    │    └─Linear: 5-11            [64, 196, 576]            111,168\n",
       "│    │    │    │    └─Softmax: 5-12           [64, 6, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-13           [64, 6, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-14            [64, 196, 192]            37,056\n",
       "│    │    │    │    └─Dropout: 5-15           [64, 196, 192]            --\n",
       "│    │    │    └─DropPath: 4-9                [1, 4, 56, 56, 192]       --\n",
       "│    │    │    └─LayerNorm: 4-10              [1, 4, 56, 56, 192]       384\n",
       "│    │    │    └─Mlp: 4-11                    [1, 4, 56, 56, 192]       --\n",
       "│    │    │    │    └─Linear: 5-16            [1, 4, 56, 56, 768]       148,224\n",
       "│    │    │    │    └─GELU: 5-17              [1, 4, 56, 56, 768]       --\n",
       "│    │    │    │    └─Dropout: 5-18           [1, 4, 56, 56, 768]       --\n",
       "│    │    │    │    └─Linear: 5-19            [1, 4, 56, 56, 192]       147,648\n",
       "│    │    │    │    └─Dropout: 5-20           [1, 4, 56, 56, 192]       --\n",
       "│    │    │    └─DropPath: 4-12               [1, 4, 56, 56, 192]       --\n",
       "│    └─PatchMerging: 2-4                      [1, 4, 28, 28, 384]       --\n",
       "│    │    └─LayerNorm: 3-3                    [1, 4, 28, 28, 768]       1,536\n",
       "│    │    └─Linear: 3-4                       [1, 4, 28, 28, 384]       294,912\n",
       "├─BasicLayer: 1-4                             [1, 768, 4, 14, 14]       --\n",
       "│    └─ModuleList: 2-5                        --                        --\n",
       "│    │    └─SwinTransformerBlock3D: 3-5       [1, 4, 28, 28, 384]       --\n",
       "│    │    │    └─LayerNorm: 4-13              [1, 4, 28, 28, 384]       768\n",
       "│    │    │    └─WindowAttention3D: 4-14      [16, 196, 384]            30,420\n",
       "│    │    │    │    └─Linear: 5-21            [16, 196, 1152]           443,520\n",
       "│    │    │    │    └─Softmax: 5-22           [16, 12, 196, 196]        --\n",
       "│    │    │    │    └─Dropout: 5-23           [16, 12, 196, 196]        --\n",
       "│    │    │    │    └─Linear: 5-24            [16, 196, 384]            147,840\n",
       "│    │    │    │    └─Dropout: 5-25           [16, 196, 384]            --\n",
       "│    │    │    └─DropPath: 4-15               [1, 4, 28, 28, 384]       --\n",
       "│    │    │    └─LayerNorm: 4-16              [1, 4, 28, 28, 384]       768\n",
       "│    │    │    └─Mlp: 4-17                    [1, 4, 28, 28, 384]       --\n",
       "│    │    │    │    └─Linear: 5-26            [1, 4, 28, 28, 1536]      591,360\n",
       "│    │    │    │    └─GELU: 5-27              [1, 4, 28, 28, 1536]      --\n",
       "│    │    │    │    └─Dropout: 5-28           [1, 4, 28, 28, 1536]      --\n",
       "│    │    │    │    └─Linear: 5-29            [1, 4, 28, 28, 384]       590,208\n",
       "│    │    │    │    └─Dropout: 5-30           [1, 4, 28, 28, 384]       --\n",
       "│    │    │    └─DropPath: 4-18               [1, 4, 28, 28, 384]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-6       [1, 4, 28, 28, 384]       --\n",
       "│    │    │    └─LayerNorm: 4-19              [1, 4, 28, 28, 384]       768\n",
       "│    │    │    └─WindowAttention3D: 4-20      [16, 196, 384]            30,420\n",
       "│    │    │    │    └─Linear: 5-31            [16, 196, 1152]           443,520\n",
       "│    │    │    │    └─Softmax: 5-32           [16, 12, 196, 196]        --\n",
       "│    │    │    │    └─Dropout: 5-33           [16, 12, 196, 196]        --\n",
       "│    │    │    │    └─Linear: 5-34            [16, 196, 384]            147,840\n",
       "│    │    │    │    └─Dropout: 5-35           [16, 196, 384]            --\n",
       "│    │    │    └─DropPath: 4-21               [1, 4, 28, 28, 384]       --\n",
       "│    │    │    └─LayerNorm: 4-22              [1, 4, 28, 28, 384]       768\n",
       "│    │    │    └─Mlp: 4-23                    [1, 4, 28, 28, 384]       --\n",
       "│    │    │    │    └─Linear: 5-36            [1, 4, 28, 28, 1536]      591,360\n",
       "│    │    │    │    └─GELU: 5-37              [1, 4, 28, 28, 1536]      --\n",
       "│    │    │    │    └─Dropout: 5-38           [1, 4, 28, 28, 1536]      --\n",
       "│    │    │    │    └─Linear: 5-39            [1, 4, 28, 28, 384]       590,208\n",
       "│    │    │    │    └─Dropout: 5-40           [1, 4, 28, 28, 384]       --\n",
       "│    │    │    └─DropPath: 4-24               [1, 4, 28, 28, 384]       --\n",
       "│    └─PatchMerging: 2-6                      [1, 4, 14, 14, 768]       --\n",
       "│    │    └─LayerNorm: 3-7                    [1, 4, 14, 14, 1536]      3,072\n",
       "│    │    └─Linear: 3-8                       [1, 4, 14, 14, 768]       1,179,648\n",
       "├─BasicLayer: 1-5                             [1, 1536, 4, 7, 7]        --\n",
       "│    └─ModuleList: 2-7                        --                        --\n",
       "│    │    └─SwinTransformerBlock3D: 3-9       [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-25              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-26      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-41            [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-42           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-43           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-44            [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-45           [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-27               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-28              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-29                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-46            [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-47              [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-48           [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-49            [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-50           [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-30               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-10      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-31              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-32      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-51            [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-52           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-53           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-54            [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-55           [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-33               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-34              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-35                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-56            [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-57              [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-58           [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-59            [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-60           [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-36               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-11      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-37              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-38      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-61            [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-62           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-63           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-64            [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-65           [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-39               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-40              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-41                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-66            [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-67              [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-68           [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-69            [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-70           [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-42               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-12      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-43              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-44      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-71            [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-72           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-73           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-74            [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-75           [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-45               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-46              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-47                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-76            [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-77              [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-78           [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-79            [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-80           [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-48               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-13      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-49              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-50      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-81            [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-82           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-83           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-84            [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-85           [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-51               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-52              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-53                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-86            [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-87              [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-88           [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-89            [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-90           [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-54               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-14      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-55              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-56      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-91            [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-92           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-93           [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-94            [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-95           [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-57               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-58              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-59                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-96            [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-97              [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-98           [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-99            [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-100          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-60               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-15      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-61              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-62      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-101           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-102          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-103          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-104           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-105          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-63               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-64              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-65                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-106           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-107             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-108          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-109           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-110          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-66               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-16      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-67              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-68      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-111           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-112          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-113          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-114           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-115          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-69               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-70              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-71                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-116           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-117             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-118          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-119           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-120          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-72               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-17      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-73              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-74      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-121           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-122          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-123          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-124           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-125          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-75               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-76              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-77                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-126           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-127             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-128          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-129           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-130          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-78               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-18      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-79              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-80      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-131           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-132          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-133          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-134           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-135          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-81               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-82              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-83                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-136           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-137             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-138          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-139           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-140          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-84               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-19      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-85              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-86      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-141           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-142          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-143          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-144           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-145          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-87               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-88              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-89                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-146           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-147             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-148          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-149           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-150          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-90               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-20      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-91              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-92      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-151           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-152          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-153          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-154           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-155          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-93               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-94              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-95                    [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-156           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-157             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-158          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-159           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-160          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-96               [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-21      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-97              [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-98      [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-161           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-162          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-163          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-164           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-165          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-99               [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-100             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-101                   [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-166           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-167             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-168          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-169           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-170          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-102              [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-22      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-103             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-104     [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-171           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-172          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-173          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-174           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-175          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-105              [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-106             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-107                   [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-176           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-177             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-178          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-179           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-180          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-108              [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-23      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-109             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-110     [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-181           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-182          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-183          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-184           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-185          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-111              [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-112             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-113                   [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-186           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-187             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-188          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-189           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-190          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-114              [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-24      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-115             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-116     [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-191           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-192          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-193          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-194           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-195          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-117              [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-118             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-119                   [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-196           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-197             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-198          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-199           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-200          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-120              [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-25      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-121             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-122     [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-201           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-202          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-203          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-204           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-205          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-123              [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-124             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-125                   [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-206           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-207             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-208          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-209           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-210          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-126              [1, 4, 14, 14, 768]       --\n",
       "│    │    └─SwinTransformerBlock3D: 3-26      [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-127             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─WindowAttention3D: 4-128     [4, 196, 768]             60,840\n",
       "│    │    │    │    └─Linear: 5-211           [4, 196, 2304]            1,771,776\n",
       "│    │    │    │    └─Softmax: 5-212          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-213          [4, 24, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-214           [4, 196, 768]             590,592\n",
       "│    │    │    │    └─Dropout: 5-215          [4, 196, 768]             --\n",
       "│    │    │    └─DropPath: 4-129              [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─LayerNorm: 4-130             [1, 4, 14, 14, 768]       1,536\n",
       "│    │    │    └─Mlp: 4-131                   [1, 4, 14, 14, 768]       --\n",
       "│    │    │    │    └─Linear: 5-216           [1, 4, 14, 14, 3072]      2,362,368\n",
       "│    │    │    │    └─GELU: 5-217             [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Dropout: 5-218          [1, 4, 14, 14, 3072]      --\n",
       "│    │    │    │    └─Linear: 5-219           [1, 4, 14, 14, 768]       2,360,064\n",
       "│    │    │    │    └─Dropout: 5-220          [1, 4, 14, 14, 768]       --\n",
       "│    │    │    └─DropPath: 4-132              [1, 4, 14, 14, 768]       --\n",
       "│    └─PatchMerging: 2-8                      [1, 4, 7, 7, 1536]        --\n",
       "│    │    └─LayerNorm: 3-27                   [1, 4, 7, 7, 3072]        6,144\n",
       "│    │    └─Linear: 3-28                      [1, 4, 7, 7, 1536]        4,718,592\n",
       "├─BasicLayer: 1-6                             [1, 1536, 4, 7, 7]        --\n",
       "│    └─ModuleList: 2-9                        --                        --\n",
       "│    │    └─SwinTransformerBlock3D: 3-29      [1, 4, 7, 7, 1536]        --\n",
       "│    │    │    └─LayerNorm: 4-133             [1, 4, 7, 7, 1536]        3,072\n",
       "│    │    │    └─WindowAttention3D: 4-134     [1, 196, 1536]            121,680\n",
       "│    │    │    │    └─Linear: 5-221           [1, 196, 4608]            7,082,496\n",
       "│    │    │    │    └─Softmax: 5-222          [1, 48, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-223          [1, 48, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-224           [1, 196, 1536]            2,360,832\n",
       "│    │    │    │    └─Dropout: 5-225          [1, 196, 1536]            --\n",
       "│    │    │    └─DropPath: 4-135              [1, 4, 7, 7, 1536]        --\n",
       "│    │    │    └─LayerNorm: 4-136             [1, 4, 7, 7, 1536]        3,072\n",
       "│    │    │    └─Mlp: 4-137                   [1, 4, 7, 7, 1536]        --\n",
       "│    │    │    │    └─Linear: 5-226           [1, 4, 7, 7, 6144]        9,443,328\n",
       "│    │    │    │    └─GELU: 5-227             [1, 4, 7, 7, 6144]        --\n",
       "│    │    │    │    └─Dropout: 5-228          [1, 4, 7, 7, 6144]        --\n",
       "│    │    │    │    └─Linear: 5-229           [1, 4, 7, 7, 1536]        9,438,720\n",
       "│    │    │    │    └─Dropout: 5-230          [1, 4, 7, 7, 1536]        --\n",
       "│    │    │    └─DropPath: 4-138              [1, 4, 7, 7, 1536]        --\n",
       "│    │    └─SwinTransformerBlock3D: 3-30      [1, 4, 7, 7, 1536]        --\n",
       "│    │    │    └─LayerNorm: 4-139             [1, 4, 7, 7, 1536]        3,072\n",
       "│    │    │    └─WindowAttention3D: 4-140     [1, 196, 1536]            121,680\n",
       "│    │    │    │    └─Linear: 5-231           [1, 196, 4608]            7,082,496\n",
       "│    │    │    │    └─Softmax: 5-232          [1, 48, 196, 196]         --\n",
       "│    │    │    │    └─Dropout: 5-233          [1, 48, 196, 196]         --\n",
       "│    │    │    │    └─Linear: 5-234           [1, 196, 1536]            2,360,832\n",
       "│    │    │    │    └─Dropout: 5-235          [1, 196, 1536]            --\n",
       "│    │    │    └─DropPath: 4-141              [1, 4, 7, 7, 1536]        --\n",
       "│    │    │    └─LayerNorm: 4-142             [1, 4, 7, 7, 1536]        3,072\n",
       "│    │    │    └─Mlp: 4-143                   [1, 4, 7, 7, 1536]        --\n",
       "│    │    │    │    └─Linear: 5-236           [1, 4, 7, 7, 6144]        9,443,328\n",
       "│    │    │    │    └─GELU: 5-237             [1, 4, 7, 7, 6144]        --\n",
       "│    │    │    │    └─Dropout: 5-238          [1, 4, 7, 7, 6144]        --\n",
       "│    │    │    │    └─Linear: 5-239           [1, 4, 7, 7, 1536]        9,438,720\n",
       "│    │    │    │    └─Dropout: 5-240          [1, 4, 7, 7, 1536]        --\n",
       "│    │    │    └─DropPath: 4-144              [1, 4, 7, 7, 1536]        --\n",
       "├─LayerNorm: 1-7                              [1, 4, 7, 7, 1536]        3,072\n",
       "├─Conv3d: 1-8                                 [1, 256, 4, 28, 28]       98,304\n",
       "├─ConvTranspose3d: 1-9                        [1, 512, 4, 28, 28]       1,573,376\n",
       "├─ConvTranspose3d: 1-10                       [1, 1024, 4, 14, 14]      6,292,480\n",
       "├─Conv3d: 1-11                                [1, 2048, 4, 7, 7]        3,145,728\n",
       "├─Conv3d: 1-12                                [1, 256, 4, 7, 7]         524,288\n",
       "├─Conv3d: 1-13                                [1, 256, 4, 7, 7]         589,824\n",
       "├─Conv3d: 1-14                                [1, 256, 4, 14, 14]       262,144\n",
       "├─Conv3d: 1-15                                [1, 256, 4, 14, 14]       589,824\n",
       "├─Conv3d: 1-16                                [1, 256, 4, 28, 28]       131,072\n",
       "├─Conv3d: 1-17                                [1, 256, 4, 28, 28]       589,824\n",
       "├─Conv3d: 1-18                                [1, 256, 4, 4, 4]         4,718,592\n",
       "├─Conv3d: 1-19                                [1, 256, 4, 2, 2]         589,824\n",
       "===============================================================================================\n",
       "Total params: 243,545,324\n",
       "Trainable params: 243,545,324\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 14.84\n",
       "===============================================================================================\n",
       "Input size (MB): 4.82\n",
       "Forward/backward pass size (MB): 1780.01\n",
       "Params size (MB): 856.06\n",
       "Estimated Total Size (MB): 2640.88\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "modelact=RetinaNet(model, args)\n",
    "\n",
    "summary(model, input_size=(1, 3, 8, 224, 224),depth=150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\belongTH\\anaconda3\\envs\\testpyt\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetFPN(\n",
      "  (patch_embed): PatchEmbed3D(\n",
      "    (proj): Conv3d(3, 192, kernel_size=(2, 4, 4), stride=(2, 4, 4))\n",
      "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (swinaclayer1): BasicLayer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): Identity()\n",
      "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.017)\n",
      "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (downsample): PatchMerging(\n",
      "      (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (swinaclayer2): BasicLayer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.035)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.052)\n",
      "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (downsample): PatchMerging(\n",
      "      (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "      (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (swinaclayer3): BasicLayer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.070)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.087)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.104)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.122)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.139)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.157)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.174)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.191)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.209)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.226)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.243)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.261)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (12): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.278)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (13): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.296)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (14): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.313)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (15): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.330)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (16): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.348)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (17): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.365)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (downsample): PatchMerging(\n",
      "      (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "      (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (swinaclayer4): BasicLayer(\n",
      "    (blocks): ModuleList(\n",
      "      (0): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.383)\n",
      "        (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): SwinTransformerBlock3D(\n",
      "        (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): WindowAttention3D(\n",
      "          (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (drop_path): DropPath(drop_prob=0.400)\n",
      "        (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate=none)\n",
      "          (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "  (upcov1): Conv3d(384, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  (upcov2): ConvTranspose3d(768, 512, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
      "  (upcov3): ConvTranspose3d(1536, 1024, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
      "  (upcov4): Conv3d(1536, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (cgru): CGRU(\n",
      "        (recurrent_conv_gates): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (recurrent_conv_out): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (cgru): CGRU(\n",
      "        (recurrent_conv_gates): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (recurrent_conv_out): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (cgru): CGRU(\n",
      "        (recurrent_conv_gates): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (recurrent_conv_out): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (cgru): CGRU(\n",
      "        (recurrent_conv_gates): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (recurrent_conv_out): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (cgru): CGRU(\n",
      "        (recurrent_conv_gates): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (recurrent_conv_out): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (cgru): CGRU(\n",
      "        (recurrent_conv_gates): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (recurrent_conv_out): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (cgru): CGRU(\n",
      "        (recurrent_conv_gates): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (recurrent_conv_out): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (cgru): CGRU(\n",
      "        (recurrent_conv_gates): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (recurrent_conv_out): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (cgru): CGRU(\n",
      "        (recurrent_conv_gates): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (recurrent_conv_out): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (cgru): CGRU(\n",
      "        (recurrent_conv_gates): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (recurrent_conv_out): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): BottleneckRCGRU(\n",
      "      (conv1): Conv3d(2048, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (conv6): Conv3d(2048, 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "  (conv7): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "  (avg_pool): AdaptiveAvgPool3d(output_size=(None, 1, 1))\n",
      "  (lateral_layer1): Conv3d(2048, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  (lateral_layer2): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  (lateral_layer3): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  (corr_layer1): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "  (corr_layer2): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "  (corr_layer3): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Training single stage FPN with OHEM, resnet as backbone')\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    args.model_perms = [3, 4, 6, 3]\n",
    "    args.model_3d_layers = [[0, 1, 2], [0, 2], [0, 2, 4], [0, 1]]\n",
    "    args.non_local_inds=[[], [], [], []]\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Args' object has no attribute 'MODEL_TYPE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\A-ROAD-CHALL\\new_video_test\\ROAD-R-2023-Challenge-main\\models\\test_end.ipynb 单元格 6\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/A-ROAD-CHALL/new_video_test/ROAD-R-2023-Challenge-main/models/test_end.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchinfo\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/A-ROAD-CHALL/new_video_test/ROAD-R-2023-Challenge-main/models/test_end.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchinfo\u001b[39;00m \u001b[39mimport\u001b[39;00m summary\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/A-ROAD-CHALL/new_video_test/ROAD-R-2023-Challenge-main/models/test_end.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m=\u001b[39mResNetFPN(BottleneckRCLSTM,args)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/A-ROAD-CHALL/new_video_test/ROAD-R-2023-Challenge-main/models/test_end.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m summary(model, input_size\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m8\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m))\n",
      "\u001b[1;32mf:\\A-ROAD-CHALL\\new_video_test\\ROAD-R-2023-Challenge-main\\models\\test_end.ipynb 单元格 6\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/A-ROAD-CHALL/new_video_test/ROAD-R-2023-Challenge-main/models/test_end.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minplanes \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/A-ROAD-CHALL/new_video_test/ROAD-R-2023-Challenge-main/models/test_end.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39msuper\u001b[39m(ResNetFPN, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/A-ROAD-CHALL/new_video_test/ROAD-R-2023-Challenge-main/models/test_end.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39;49mMODEL_TYPE\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mSlowFast\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/A-ROAD-CHALL/new_video_test/ROAD-R-2023-Challenge-main/models/test_end.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mconfigs/ROAD.yml\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/A-ROAD-CHALL/new_video_test/ROAD-R-2023-Challenge-main/models/test_end.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         config \u001b[39m=\u001b[39m yaml\u001b[39m.\u001b[39mload(f, Loader\u001b[39m=\u001b[39myaml\u001b[39m.\u001b[39mFullLoader)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Args' object has no attribute 'MODEL_TYPE'"
     ]
    }
   ],
   "source": [
    "# 加载Swin Transformer的权重\n",
    "import torch\n",
    "from torchvision.models import resnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载neck和head的权重\n",
    "neck_head_weights = torch.load('F:/A-ROAD-CHALL/weights/weights_task1/pretrained_weights_task1.pth')\n",
    "\n",
    "# 只取neck和head部分的权重\n",
    "neck_head_weights = {k: v for k, v in neck_head_weights.items() if k.startswith('layer1') or k.startswith('corr') or k.startswith('conv6') or k.startswith('conv7')}\n",
    "\n",
    "# 加载到模型中\n",
    "model.load_state_dict(neck_head_weights, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "swinaclayer1\n",
      "2\n",
      "swinaclayer2\n",
      "3\n",
      "swinaclayer3\n",
      "4\n",
      "swinaclayer4\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    layer_name = 'swinaclayer{}'.format(i)\n",
    "    print(i)\n",
    "    print(layer_name)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testpyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
